{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains the Paragraph Vector DM and DBOW models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import pickle\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "from datetime import datetime, timedelta\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '/path/to/data/dir'\n",
    "file_pcs = path.join(dir_data, 'patient_code_sequences.txt')\n",
    "file_persons = path.join(dir_data, 'persons.csv')\n",
    "file_concepts = path.join(dir_data, 'concepts.csv')\n",
    "file_sequences = path.join(dir_data, 'patient_sequences.pkl')\n",
    "file_backup_suffix = '.backup'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the persons\n",
    "df_persons = pd.read_csv(file_persons, sep='\\t', header=0, index_col=0, \n",
    "                         parse_dates=['birth_date'], infer_datetime_format=True)\n",
    "\n",
    "# Load the concept definitions\n",
    "df_concepts = pd.read_csv(file_concepts, sep='\\t', header=0, index_col='concept_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load patient sequences into TaggedDocuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for reading in the patient_code_sequences.txt\n",
    "\n",
    "# Date of occurrence and list of concepts occurring on this date\n",
    "DateOccurrence = namedtuple('DateOccurrence', ['date', 'concept_ids'])\n",
    "\n",
    "def _process_pcs_line(line):\n",
    "    \"\"\" Processes a line from patient_code_sequences.txt and parses out the patient ID\n",
    "    and DateOccurrences \"\"\"\n",
    "    split = line.strip().split('\\t')\n",
    "        \n",
    "    # person_id is the first entry\n",
    "    pid = int(split.pop(0))\n",
    "    \n",
    "    # Process the remaining string into a list of Occurrences\n",
    "    date_occurrences = [_process_date_occurrence_str(x) for x in split]\n",
    "    \n",
    "    return pid, date_occurrences\n",
    "\n",
    "def _process_date_occurrence_str(dos):\n",
    "    \"\"\" Processes a DateOccurrence string \n",
    "    format: YYYY-MM-DD:<list of concept IDs separated by commas> \"\"\"\n",
    "    date_str, concept_ids_str = dos.split(':')\n",
    "    occ = DateOccurrence(datetime.strptime(date_str.strip(), '%Y-%m-%d'), \n",
    "                         [int(x) for x in concept_ids_str.split(',')])\n",
    "    return occ\n",
    "\n",
    "def create_patient_sequences(f_pcs_in, f_seq_out=None, min_seq_length=10, randomize_order=True, verbose=False, save_intermediates=False): \n",
    "    \"\"\" Reads the patient_code_sequences.txt file and parses it into sequences for each patient\n",
    "    \n",
    "    Note: save_intermediates makes it a lot slower \"\"\"\n",
    "\n",
    "    # For keeping track of processing time\n",
    "    t1 = time()\n",
    "\n",
    "    # pseqs - list of TaggedDocument(words=[concept_ids], tags=[person_id])\n",
    "    pseqs = list()\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    if f_seq_out:\n",
    "        f_intermediate = f_seq_out + '.tmp'\n",
    "    \n",
    "    # Read patient_code_sequences.txt\n",
    "    with open(f_pcs_in) as fh:  \n",
    "        # Skip the heaer line\n",
    "        fh.readline()\n",
    "        \n",
    "        for line in fh:\n",
    "            # Parse the line into person_id and list of date_occurrences\n",
    "            pid, date_occurrences = _process_pcs_line(line)\n",
    "\n",
    "            # Combine sequence of concepts from each date into on sequence for the patient\n",
    "            current_seq = []\n",
    "            for date_occurrence in date_occurrences:\n",
    "                concepts = date_occurrence.concept_ids\n",
    "                if randomize_order:\n",
    "                    # Randomize the order of concepts occurring on the same date. Shuffle is applied in place\n",
    "                    shuffle(concepts)\n",
    "                    \n",
    "                current_seq += concepts\n",
    "                \n",
    "            if len(current_seq) >= min_seq_length:\n",
    "                pseqs.append(TaggedDocument(words=[str(x) for x in current_seq], tags=[pid]))\n",
    "\n",
    "            # Display progress\n",
    "            count += 1\n",
    "            if count % 100000 == 0:\n",
    "                if verbose: \n",
    "                    # Processing time and size of data structure\n",
    "                    ellapsed_time = (time() - t1) / 60\n",
    "                    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "\n",
    "                if save_intermediates and f_seq_out:\n",
    "                    # Save a backup copy of the data\n",
    "                    pickle.dump(pseqs, open(f_intermediate, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)      \n",
    "\n",
    "    if f_seq_out:\n",
    "        # Save the concept age distributions            \n",
    "        pickle.dump(pseqs, open(f_seq_out, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Delete the backup file\n",
    "        if save_intermediates and path.exists(f_intermediate):\n",
    "            os.remove(f_intermediate)\n",
    "\n",
    "    # Display overall processing time\n",
    "    ellapsed_time = (time() - t1) / 60\n",
    "    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "    \n",
    "    return pseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseqs = create_patient_sequences(file_pcs, f_seq_out=None, min_seq_length=5, randomize_order=True, \n",
    "                                        verbose=True, save_intermediates=False)\n",
    "n_pseqs = len(pseqs)\n",
    "print(n_pseqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_filename(model, epochs=None):\n",
    "    \"\"\" Generate a filename for to save the model using the string representation of the model, \n",
    "    which already includes most of the important model parameters. \"\"\"\n",
    "    f_model = re.sub('[^\\w\\-_\\. ]', '_', str(model))\n",
    "    if epochs:\n",
    "        f_model += f'e{epochs}'\n",
    "    f_model += datetime.now().strftime(\"_%Y-%m-%d\")\n",
    "    f_model += '.d2v'\n",
    "    return f_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paragraph Vector - Distributed Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm = Doc2Vec(dm=1, vector_size=100, window=7, min_count=5, alpha=0.023, hs = 0, negative=15, \n",
    "                   epochs=20, workers=6, report_delay=60)\n",
    "\n",
    "# Build Vocab\n",
    "t1 = time()\n",
    "model_dm.build_vocab(pseqs, progress_per=1000000)\n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'Build Vocab Ellapsed Time: {ellapsed_time} min')\n",
    "\n",
    "# Train\n",
    "t1 = time()\n",
    "model_dm.train(pseqs, total_examples=model_dm.corpus_count, epochs=model_dm.epochs, report_delay=60)\n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'Train Ellapsed Time: {ellapsed_time} min')\n",
    "\n",
    "# Save the model\n",
    "f_model = path.join(dir_data, model_filename(model_dm, epochs=model_dm.epochs))\n",
    "print(f'Saving model to: {f_model}')\n",
    "model_dm.save(f_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paragraph Vector - Distributed Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=100, window=7, min_count=5, alpha=0.023, hs = 0, negative=15, \n",
    "                     epochs=20, workers=6, report_delay=60)\n",
    "\n",
    "# Build Vocab\n",
    "t1 = time()\n",
    "model_dbow.build_vocab(pseqs, progress_per=100000)\n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'Build Vocab Ellapsed Time: {ellapsed_time} min')\n",
    "\n",
    "# Train\n",
    "t1 = time()\n",
    "model_dbow.train(pseqs, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs, report_delay=60)\n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'Train Ellapsed Time: {ellapsed_time} min')\n",
    "\n",
    "# Save the model\n",
    "f_model = path.join(dir_data, model_filename(model_dbow, epochs=model_dbow.epochs))\n",
    "print(f'Saving model to: {f_model}')\n",
    "model_dbow.save(f_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
