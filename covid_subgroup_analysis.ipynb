{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COVID Subgroup Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path, makedirs\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "from datetime import datetime, timedelta, date\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt, transforms as mtransforms\n",
    "from random import Random\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import getpass\n",
    "import pyodbc\n",
    "import sqlalchemy\n",
    "from scipy.stats import chi2_contingency, mannwhitneyu\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '/path/to/data/dir'\n",
    "dir_model = '/path/to/model/dir'\n",
    "# input\n",
    "file_pcs = path.join(dir_data, 'patient_code_sequences.txt')\n",
    "file_events = path.join(dir_data, 'events_covid19.csv')\n",
    "file_concepts = path.join(dir_data, 'concepts.csv')\n",
    "file_model_dm = path.join(dir_model, 'doc2vec_dm_model.d2v')\n",
    "file_model_dbow = path.join(dir_model, 'doc2vec_dbow_model.d2v')\n",
    "\n",
    "# output\n",
    "timestamp_str = datetime.now().strftime('%y%m%d_%H%M')\n",
    "dir_output = path.join(dir_data, timestamp_str)\n",
    "makedirs(dir_output)\n",
    "file_backup_suffix = '.backup'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd=getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL server config\n",
    "sql_config = {\n",
    "    'driver': 'ODBC Driver 17 for SQL Server',\n",
    "    'server': 'sql.server.host',\n",
    "    'database': 'database_name',\n",
    "    'uid': 'user_name'\n",
    "}\n",
    "\n",
    "conn = pyodbc.connect(**sql_config, pwd=pwd)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_url = sqlalchemy.engine.URL.create(\n",
    "    \"mssql+pyodbc\",\n",
    "    username=\"username\",\n",
    "    password=pwd,\n",
    "    host=\"sql.server.host\",\n",
    "    database=\"database_name\",\n",
    "    query={\n",
    "        \"driver\": \"ODBC Driver 17 for SQL Server\"\n",
    "    },\n",
    ")\n",
    "engine = sqlalchemy.create_engine(connection_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window_covid = [0,28]\n",
    "time_window_pre = [-730, -22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the concept definitions\n",
    "df_concepts = pd.read_csv(file_concepts, sep='\\t', header=0, index_col='concept_id', low_memory=False)\n",
    "df_concepts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the index events\n",
    "df_events_orig = pd.read_csv(file_events, sep='\\t', header=0, index_col='person_id')\n",
    "df_events_orig['start_date'] = df_events_orig['start_date'].apply(date.fromisoformat)\n",
    "df_events_orig['end_date'] = df_events_orig['end_date'].apply(date.fromisoformat)\n",
    "df_events_orig['op_start_date'] = df_events_orig['op_start_date'].apply(date.fromisoformat)\n",
    "df_events_orig['op_end_date'] = df_events_orig['op_end_date'].apply(date.fromisoformat)\n",
    "df_events_orig.sort_values(by='start_date', inplace=True)\n",
    "\n",
    "print(len(df_events_orig))\n",
    "df_events_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the PV models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm = Doc2Vec.load(file_model_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load(file_model_dbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the cohort for disease subtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for reading in the patient_code_sequences.txt\n",
    "\n",
    "# Date of occurrence and list of concepts occurring on this date\n",
    "DateOccurrence = namedtuple('DateOccurrence', ['date', 'concept_ids'])\n",
    "\n",
    "def _process_pcs_line(line):\n",
    "    \"\"\" Processes a line from patient_code_sequences.txt and parses out the patient ID\n",
    "    and DateOccurrences \"\"\"\n",
    "    split = line.strip().split('\\t')\n",
    "        \n",
    "    # person_id is the first entry\n",
    "    pid = int(split.pop(0))\n",
    "    \n",
    "    # Process the remaining string into a list of Occurrences\n",
    "    date_occurrences = [_process_date_occurrence_str(x) for x in split]\n",
    "    \n",
    "    return pid, date_occurrences\n",
    "\n",
    "def _process_date_occurrence_str(dos):\n",
    "    \"\"\" Processes a DateOccurrence string \n",
    "    format: YYYY-MM-DD:<list of concept IDs separated by commas> \"\"\"\n",
    "    date_str, concept_ids_str = dos.split(':')\n",
    "    occ = DateOccurrence(date.fromisoformat(date_str), \n",
    "                         [int(x) for x in concept_ids_str.split(',')])\n",
    "    return occ\n",
    "\n",
    "def create_patient_sequences(f_pcs_in, f_seq_out=None, min_seq_length=10, randomize_order=True, random_seed=None, verbose=False, save_intermediates=False): \n",
    "    \"\"\" Reads the patient_code_sequences.txt file and parses it into sequences for each patient\n",
    "    \n",
    "    Note: save_intermediates makes it a lot slower \"\"\"\n",
    "\n",
    "    # For keeping track of processing time\n",
    "    t1 = time()\n",
    "\n",
    "    # pseqs - list of TaggedDocument(words=[concept_ids], tags=[person_id])\n",
    "    pseqs = list()\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    r = Random(random_seed)\n",
    "    \n",
    "    if f_seq_out:\n",
    "        f_intermediate = f_seq_out + '.tmp'\n",
    "    \n",
    "    # Read patient_code_sequences.txt\n",
    "    with open(f_pcs_in) as fh:  \n",
    "        # Skip the heaer line\n",
    "        fh.readline()\n",
    "        \n",
    "        for line in fh:\n",
    "            # Parse the line into person_id and list of date_occurrences\n",
    "            pid, date_occurrences = _process_pcs_line(line)\n",
    "\n",
    "            # Combine sequence of concepts from each date into on sequence for the patient\n",
    "            current_seq = []\n",
    "            for date_occurrence in date_occurrences:\n",
    "                concepts = date_occurrence.concept_ids\n",
    "                if randomize_order:\n",
    "                    # Randomize the order of concepts occurring on the same date. Shuffle is applied in place\n",
    "                    r.shuffle(concepts)\n",
    "                    \n",
    "                current_seq += concepts\n",
    "                \n",
    "            if len(current_seq) >= min_seq_length:\n",
    "                pseqs.append(TaggedDocument(words=[str(x) for x in current_seq], tags=[pid]))\n",
    "\n",
    "            # Display progress\n",
    "            count += 1\n",
    "            if count % 100000 == 0:\n",
    "                if verbose: \n",
    "                    # Processing time and size of data structure\n",
    "                    ellapsed_time = (time() - t1) / 60\n",
    "                    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "\n",
    "                if save_intermediates and f_seq_out:\n",
    "                    # Save a backup copy of the data\n",
    "                    pickle.dump(pseqs, open(f_intermediate, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)      \n",
    "\n",
    "    if f_seq_out:\n",
    "        # Save the concept age distributions            \n",
    "        pickle.dump(pseqs, open(f_seq_out, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Delete the backup file\n",
    "        if save_intermediates and path.exists(f_intermediate):\n",
    "            os.remove(f_intermediate)\n",
    "\n",
    "    # Display overall processing time\n",
    "    ellapsed_time = (time() - t1) / 60\n",
    "    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "    \n",
    "    return pseqs\n",
    "\n",
    "# Class for storing info about a cohort patient sequence\n",
    "# Sequence stored as TaggedDocument object for D2V processing\n",
    "# Will use OMOP concept ID for label\n",
    "CohortPatientSeq = namedtuple('CohortPatientSeq', ['person_id', 'sequence', 'label', 'date_lower', 'date_upper'])\n",
    "\n",
    "def create_cohort_patient_sequences(f_pcs_in, df_events, cohort_concept=37311061, f_seq_out=None, inclusion_window=(None, None), time_window=[-14,28], event_end=False, \n",
    "                                    randomize_order=True, random_seed=None, verbose=False, save_intermediates=False): \n",
    "    \"\"\" Reads the patient_code_sequences.txt file and extracts sequences within the time_window days around the \n",
    "    first occurrence of any encountered desired concept\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f_pcs_in: filename of patient code sequences file\n",
    "    df_events: dataframe of events\n",
    "    cohort_concpets: not really used for COVID-19 analysis; default 37311061\n",
    "    f_seq_out: filename to write sequences to\n",
    "    inclusion_window: range of event start dates that are included: (date range beginning, date range ending). None implies no restrictions applied. Default: (None, None)\n",
    "    time_window: relative date range to include for each patient, relative to index date (hospital start date): [date range beginnning, date range ending]\n",
    "    event_end: If true, will use the max(event end date, time_window[1]) as the end date. If false, will use time_window[1] as the end date.\n",
    "    randomize_order: If true, randomizes order of codes per day\n",
    "    verbose: If true, prints verbose progress\n",
    "    save_intermediate: save intermediate work files. Note: this makes it a lot slower \n",
    "    \n",
    "    Return \n",
    "    ------\n",
    "    cpss: list[CohortPatientSequence]\"\"\"\n",
    "\n",
    "    # For keeping track of processing time\n",
    "    t1 = time()\n",
    "\n",
    "    # pcss - cohort patient sequences: list of CohortPatientSeq objects\n",
    "    cpss = dict()\n",
    "    count = 0\n",
    "    \n",
    "    # Time window for finding occurrences \n",
    "    time_window_pre = timedelta(days=time_window[0])\n",
    "    time_window_post = timedelta(days=time_window[1])\n",
    "    \n",
    "    # Event inclusion window\n",
    "    if inclusion_window is None or len(inclusion_window) != 2:\n",
    "        print('Warning, inclusion window expected to be tuple of length 2. Proceeding with default value.')\n",
    "        inclusion_window = (None, None)\n",
    "    else:\n",
    "        inclusion_window = tuple([date.fromisoformat(d) if type(d) is str else d for d in inclusion_window])        \n",
    "    \n",
    "    r = Random(random_seed)\n",
    "    \n",
    "    if f_seq_out:\n",
    "        f_intermediate = f_seq_out + '.tmp'\n",
    "    \n",
    "    # Read patient_code_sequences.txt\n",
    "    with open(f_pcs_in) as fh:  \n",
    "        # Skip the header line\n",
    "        fh.readline()\n",
    "        \n",
    "        for line in fh:\n",
    "            # Parse the line into person_id and list of date_occurrences\n",
    "            pid, date_occurrences = _process_pcs_line(line)\n",
    "            \n",
    "            if pid not in df_events.index:\n",
    "                # Couldn't find this person_id in the events table\n",
    "                print(f\"Could not find person_id {pid} in df_events\")\n",
    "                continue\n",
    "                \n",
    "            # Get the index date from events table    \n",
    "            event_start_date = df_events.loc[pid, 'start_date']\n",
    "            event_end_date = df_events.loc[pid, 'end_date']\n",
    "\n",
    "            # Check event date inclusion criteria\n",
    "            if ((inclusion_window[0] is not None and event_start_date < inclusion_window[0]) or \n",
    "                    (inclusion_window[1] is not None and event_start_date > inclusion_window[1])):\n",
    "                # event was outside inclusion window\n",
    "                print(f'{event_start_date} outside of inclusion window')\n",
    "                continue\n",
    "    \n",
    "            date_lower = event_start_date + time_window_pre\n",
    "            date_upper = event_start_date + time_window_post\n",
    "            if event_end:                                \n",
    "                # Use the max of the upper limit as specified by the time window or the hospitalization end date\n",
    "                date_upper = max(date_upper, event_end_date)\n",
    "            \n",
    "            current_seq = list()\n",
    "            for do in date_occurrences:\n",
    "                if do.date < date_lower:\n",
    "                    continue\n",
    "\n",
    "                if do.date > date_upper:\n",
    "                    # No more date_occurrences within the desired time window.                         \n",
    "                    break\n",
    "\n",
    "                # The date_occurrence is within the time_window. Add occurrences to seq\n",
    "                concepts = do.concept_ids\n",
    "                if randomize_order:\n",
    "                    # Randomize the order of concepts occurring on the same date. Shuffle is applied in place\n",
    "                    r.shuffle(concepts)\n",
    "                current_seq += concepts\n",
    "\n",
    "            # Convert the sequence of OMOP concept IDs to TaggedDocument for D2V processing\n",
    "            tagged_doc_seq = TaggedDocument(words=[str(x) for x in current_seq], tags=[pid])                \n",
    "\n",
    "            # Save the sequence along with patient ID, time window, and label\n",
    "            cps = CohortPatientSeq(pid, tagged_doc_seq, cohort_concept, date_lower, date_upper)\n",
    "            cpss[pid] = cps\n",
    "\n",
    "            # Display progress\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                if verbose: \n",
    "                    # Processing time\n",
    "                    ellapsed_time = (time() - t1) / 60\n",
    "                    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "\n",
    "                if save_intermediates and f_seq_out:\n",
    "                    # Save a backup copy of the data\n",
    "                    pickle.dump(cpss, open(f_intermediate, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)      \n",
    "\n",
    "    if f_seq_out:\n",
    "        # Save         \n",
    "        pickle.dump(cpss, open(f_seq_out, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Delete the backup file\n",
    "        if save_intermediates and path.exists(f_intermediate):\n",
    "            os.remove(f_intermediate)\n",
    "\n",
    "    # Display overall processing time\n",
    "    ellapsed_time = (time() - t1) / 60\n",
    "    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "    \n",
    "    return cpss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID-19\n",
    "\n",
    "37311061 - Disease caused by severe acute respiratory syndrome coronavirus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_sequences = path.join(dir_output, 'covid_cohort_inpatient_sequences.pkl')\n",
    "cpss_orig = create_cohort_patient_sequences(file_pcs, df_events_orig, f_seq_out=file_sequences, inclusion_window=('2020-03-01', None), time_window=time_window_covid, \n",
    "                                            event_end=True, randomize_order=True, random_seed=42, verbose=True, save_intermediates=False)\n",
    "n_cpss_orig = len(cpss_orig)\n",
    "print(n_cpss_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also get patient sequences prior to their COVID diagnosis so that we can evaluate their baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_sequences_pre = path.join(dir_output, 'covid_cohort_inpatient_sequences_pre.pkl')\n",
    "cpss_pre_orig = create_cohort_patient_sequences(file_pcs, df_events_orig, f_seq_out=file_sequences_pre, time_window=time_window_pre, \n",
    "                                       randomize_order=True, random_seed=42, verbose=True, save_intermediates=False)\n",
    "n_cpss_pre_orig = len(cpss_pre_orig)\n",
    "print(n_cpss_pre_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of distinct patients\n",
    "patient_ids_orig = list(cpss_orig.keys())\n",
    "n_patients_orig = len(patient_ids_orig)\n",
    "print(n_patients_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim down df_events to the patients that we have data for\n",
    "print(len(df_events_orig))\n",
    "df_events_orig = df_events_orig[df_events_orig.index.isin(patient_ids_orig)]\n",
    "print(len(df_events_orig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterize the patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographics(df_events):\n",
    "    sql = f\"\"\"\n",
    "        SELECT c_g.concept_name AS sex, convert(date, birth_datetime) AS birth_date, \n",
    "            CASE \n",
    "                WHEN race_concept_id IN (\n",
    "                    8557, -- Native Hawaiian or Other Pacific Islander\n",
    "                    8657, -- American Indian or Alaska Native                    \n",
    "                    8515 -- Asians\n",
    "                    ) THEN 'Asian, AIAN, or NHPI'\n",
    "                WHEN race_concept_id NOT IN (\n",
    "                    0, -- no matching concept\n",
    "                    8522, -- other race\n",
    "                    8552, -- unknown\n",
    "                    44814653 -- unknown\n",
    "                    ) THEN c_r.concept_name                                             \n",
    "                WHEN ethnicity_concept_id = 38003563 THEN 'Hispanic'\n",
    "                WHEN ethnicity_concept_id IN (\n",
    "                    0, -- no matching concept\n",
    "                    38003564 -- not hispanic or latino\n",
    "                    ) THEN 'Other or Unknown'\n",
    "                ELSE \n",
    "                    -- Red flag to indicate we didn't account for some combination\n",
    "                    CONCAT(str(race_concept_id), '-', str(ethnicity_concept_id))\n",
    "            END AS ethnicity_race      \n",
    "        FROM person p \n",
    "        JOIN concept c_g ON p.gender_concept_id = c_g.concept_id\n",
    "        JOIN concept c_r ON p.race_concept_id = c_r.concept_id\n",
    "        WHERE person_id = ?\n",
    "        \"\"\"    \n",
    "    data_demogs = list()\n",
    "    for person_id, row in df_events.iterrows():\n",
    "        df = cursor.execute(sql, person_id)          \n",
    "        patient_demo_row = cursor.fetchone()                        \n",
    "        age = (row['start_date'] - patient_demo_row[1]).days / 365.2425  # Calculate age at event\n",
    "        data_demogs.append([patient_demo_row[0], age, patient_demo_row[2]])                \n",
    "                        \n",
    "    return pd.DataFrame(data_demogs, columns=['sex', 'age', 'race_ethnicity'], index=df_events.index)              \n",
    "\n",
    "def visit_characteristics(df_events, time_window_pre_covid, time_window_covid):\n",
    "    # Count how many visits each patient had in the pre-COVID and COVID time windows\n",
    "    sql_visit_count = f\"\"\"SELECT COUNT(*) AS visit_count\n",
    "    FROM visit_occurrence\n",
    "    WHERE person_id = ? AND visit_start_date BETWEEN ? AND ?;\n",
    "    \"\"\"    \n",
    "    data_visits = list()\n",
    "    for person_id, row in df_events.iterrows():\n",
    "        event_start_date = row['start_date']        \n",
    "\n",
    "        # Count pre-COVID visits\n",
    "        date_lower_pre = event_start_date + timedelta(days=time_window_pre_covid[0])\n",
    "        date_upper_pre = event_start_date + timedelta(days=time_window_pre_covid[1])\n",
    "        cursor.execute(sql_visit_count, [person_id, date_lower_pre, date_upper_pre])\n",
    "        count_pre = cursor.fetchone()[0]\n",
    "\n",
    "        # Count COVID visits\n",
    "        date_lower_covid = event_start_date + timedelta(days=time_window_covid[0])\n",
    "        date_upper_covid = event_start_date + timedelta(days=time_window_covid[1])\n",
    "        cursor.execute(sql_visit_count, [person_id, date_lower_covid, date_upper_covid])\n",
    "        count_covid = cursor.fetchone()[0]\n",
    "\n",
    "        data_visits.append([count_pre, count_covid])\n",
    "    \n",
    "    df = pd.DataFrame(data_visits, columns=['visit_count_pre_covid', 'visit_count_covid'], index=df_events.index)\n",
    "    \n",
    "    df['event_start_date'] = df_events['start_date']\n",
    "    \n",
    "    # Calculate the length of stay of the event visit\n",
    "    df['event_visit_los'] = df_events.apply(lambda x: (x.end_date - x.start_date).days + 1, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def vaccination_rate(df_events, show=False):\n",
    "    # Get percent of persons with full vaccination status before the start of their hospitalization\n",
    "    n_persons = len(df_events)\n",
    "    # This table has unique patients who are fully vaccinated and their earliest full vaccination status\n",
    "    sql = \"\"\"SELECT * FROM database_name.dbo.covid19_vaccination;\"\"\"\n",
    "    df = pd.read_sql(sql, engine)\n",
    "    df.set_index('person_id', inplace=True)    \n",
    "    df = df_events.join(df, rsuffix='_vax')\n",
    "    df = df[df.end_date_vax < df.start_date]\n",
    "    n_vax = len(df)\n",
    "    p_vax = n_vax / n_persons * 100\n",
    "    if show:\n",
    "        print(f'Vaccination rate: {n_vax} / {n_persons} ({p_vax}%)')\n",
    "    return p_vax\n",
    "\n",
    "def plot_sex(sex, sex_bg=None, show=True, show_ylabel=True):    \n",
    "    width = 0.7\n",
    "    offset = 0\n",
    "    x = np.array(range(2))\n",
    "    if sex_bg is not None:\n",
    "        width = 0.35\n",
    "        offset = width / 2       \n",
    "    \n",
    "    df = pd.DataFrame(sex.value_counts(normalize=True)*100)    \n",
    "    \n",
    "    if sex_bg is not None:\n",
    "        df_bg = pd.DataFrame(sex_bg.value_counts(normalize=True)*100)\n",
    "        df_bg.columns = ['full']\n",
    "        df.columns = ['cluster']\n",
    "        df = df.join(df_bg)\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    df.plot(kind='bar', color=['#1F77B4', '#AAAAAA'], ax=ax)\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    if show_ylabel:\n",
    "        plt.ylabel('Patients (%)')\n",
    "    ax.get_legend().remove()  \n",
    "    plt.xlabel('Sex')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "def plot_race(race, race_bg=None, show=True, show_ylabel=True):    \n",
    "    width = 0.7\n",
    "    offset = 0\n",
    "    x = np.array(range(2))\n",
    "    if race_bg is not None:\n",
    "        width = 0.35\n",
    "        offset = width / 2       \n",
    "    \n",
    "    name_hispanic = 'Hispanic'\n",
    "    name_asian_aian_nhpi = 'Asian\\nAIAN\\nNHPI'\n",
    "    name_black = 'Black'\n",
    "    name_white ='White'\n",
    "    name_unknown = 'Other\\nUnknown'    \n",
    "    df = pd.DataFrame(race.value_counts(normalize=True)*100)    \n",
    "    df = df.rename(index={\n",
    "        'Other or Unknown': name_unknown,\n",
    "        'Black or African American': name_black,\n",
    "        'Asian, AIAN, or NHPI': name_asian_aian_nhpi\n",
    "    }).reindex([name_hispanic, name_asian_aian_nhpi, name_black, name_white, name_unknown])\n",
    "    \n",
    "    if race_bg is not None:\n",
    "        df_bg = pd.DataFrame(race_bg.value_counts(normalize=True)*100)\n",
    "        df_bg = df_bg.rename(index={\n",
    "            'Other or Unknown': name_unknown,\n",
    "            'Black or African American': name_black,\n",
    "            'Asian, AIAN, or NHPI': name_asian_aian_nhpi\n",
    "        }).reindex([name_hispanic, name_asian_aian_nhpi, name_black, name_white, name_unknown])\n",
    "        df_bg.columns = ['full']\n",
    "        df.columns = ['cluster']\n",
    "        df = df.join(df_bg)\n",
    "        \n",
    "    ax = plt.gca()\n",
    "    df.plot(kind='bar', color=['#1F77B4', '#AAAAAA'], ax=ax)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    if show_ylabel:\n",
    "        plt.ylabel('Patients (%)')\n",
    "    ax.get_legend().remove()  \n",
    "    plt.xlabel('Race-ethnicity')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()        \n",
    "\n",
    "def plot_age_histogram(age, age_bg=None, bins=None, ranged_bin_labels=False, skip_xticks=False, show=True):\n",
    "    if bins is None:\n",
    "        bins = np.append(np.arange(17)*5, np.Inf)\n",
    "\n",
    "    # Create bin labels\n",
    "    \n",
    "    if ranged_bin_labels:\n",
    "        # Ranged bin labels\n",
    "        bin_labels = list()\n",
    "        for i in range(len(bins) - 1):\n",
    "            lower = bins[i]\n",
    "            upper = bins[i+1]\n",
    "            if lower < upper - 1:\n",
    "                if upper == np.Inf:\n",
    "                    bin_labels.append(f'{lower:.0f}+')\n",
    "                else:\n",
    "                    bin_labels.append(f'{lower:.0f}-{(upper-1):.0f}')\n",
    "            else:\n",
    "                bin_labels.append(f'{lower:.0f}')\n",
    "    else:\n",
    "        # Bin labels using just the left edge (more compact labels)\n",
    "        bin_labels = [f'{x:.0f}' for x in bins][:-1]\n",
    "        bin_labels[-1] += '+'        \n",
    "            \n",
    "    width = 0.7\n",
    "    offset = 0\n",
    "    x = np.array(range(len(bins)-1))\n",
    "    if age_bg is not None:\n",
    "        width = 0.35\n",
    "        offset = width / 2   \n",
    "        \n",
    "    hist, bin_edges = np.histogram(age, bins=bins)\n",
    "    plt.bar(x=x-offset, height=hist/len(age)*100, tick_label=bin_labels, width=width)\n",
    "    plt.xlabel('Age (years)')\n",
    "    plt.ylabel('Patients (%)')\n",
    "    \n",
    "    xticks=x\n",
    "    if skip_xticks:\n",
    "        xticks = xticks[0:len(bin_labels):2]\n",
    "        bin_labels = bin_labels[0:len(bin_labels):2]\n",
    "    plt.xticks(xticks, labels=bin_labels)\n",
    "    \n",
    "    if age_bg is not None:\n",
    "        hist, bin_edges = np.histogram(age_bg, bins=bins)\n",
    "        plt.bar(x=x+offset, height=hist/len(age_bg)*100, color='gray', width=width)  \n",
    "        plt.legend(['cluster', 'full'], loc='best')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_visit_counts(visit_counts, visit_counts_bg=None, bins=None, xlabel='Number of Visits', show=True, show_ylabel=True):\n",
    "    if bins is None:\n",
    "        bins = [0, 1, 3, 5, 10, 20, 50, 100, np.Inf]\n",
    "\n",
    "    # Create bin labels\n",
    "    bin_labels = []\n",
    "    for i in range(len(bins) - 1):\n",
    "        lower = bins[i]\n",
    "        upper = bins[i+1]\n",
    "        if lower < upper - 1:\n",
    "            if upper == np.Inf:\n",
    "                bin_labels.append(f'{lower}+')\n",
    "            else:\n",
    "                bin_labels.append(f'{lower}-{upper-1}')\n",
    "        else:\n",
    "            bin_labels.append(str(lower))\n",
    "            \n",
    "    width = 0.7\n",
    "    offset = 0\n",
    "    x = np.array(range(len(bins)-1))\n",
    "    if visit_counts_bg is not None:\n",
    "        width = 0.35\n",
    "        offset = width / 2   \n",
    "            \n",
    "    hist, bin_edges = np.histogram(visit_counts, bins=bins)\n",
    "    plt.bar(x=x-offset, height=hist/len(visit_counts)*100, tick_label=bin_labels, width=width)\n",
    "    plt.xlabel(xlabel)\n",
    "    if show_ylabel:\n",
    "        plt.ylabel('Patients (%)')\n",
    "    plt.xticks(x, rotation=90)\n",
    "    \n",
    "    if visit_counts_bg is not None:\n",
    "        hist, bin_edges = np.histogram(visit_counts_bg, bins=bins)\n",
    "        plt.bar(x=x+offset, height=hist/len(visit_counts_bg)*100, color='#AAAAAA', width=width)    \n",
    "        plt.legend(['cluster', 'full'], loc='best')\n",
    "        \n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    return hist, bin_edges\n",
    "    \n",
    "def plot_precovid_visit_counts(visit_counts, visit_counts_bg=None, show=True, show_ylabel=True):\n",
    "    bins = [0, 1, 3, 5, 10, 20, 50, 100, np.Inf]\n",
    "    return plot_visit_counts(visit_counts, visit_counts_bg, bins=bins, xlabel='Pre-COVID number of visits', show=show, show_ylabel=show_ylabel)\n",
    "\n",
    "def plot_covid_visit_counts(visit_counts, visit_counts_bg=None, show=True, show_ylabel=True):\n",
    "    bins = [1, 2, 3, 5, 7, 10, np.Inf]\n",
    "    return plot_visit_counts(visit_counts, visit_counts_bg, bins=bins, xlabel='COVID number of visits', show=show, show_ylabel=show_ylabel)\n",
    "    \n",
    "def plot_los(los, los_bg=None, bins=None, show=True, show_ylabel=True):                \n",
    "    if bins is None:\n",
    "            bins = [1, 3, 5, 10, 20, 50, 100, np.Inf]\n",
    "            \n",
    "    # Create bin labels\n",
    "    bin_labels = []\n",
    "    for i in range(len(bins) - 1):\n",
    "        lower = bins[i]\n",
    "        upper = bins[i+1]\n",
    "        if lower < upper - 1:\n",
    "            if upper == np.Inf:\n",
    "                bin_labels.append(f'{lower}+')\n",
    "            else:\n",
    "                bin_labels.append(f'{lower}-{upper-1}')\n",
    "        else:\n",
    "            bin_labels.append(str(lower))\n",
    "    \n",
    "    width = 0.7\n",
    "    offset = 0\n",
    "    x = np.array(range(len(bins)-1))\n",
    "    if los_bg is not None:\n",
    "        width = 0.35\n",
    "        offset = width / 2        \n",
    "    \n",
    "    hist, bin_edges = np.histogram(los, bins=bins)    \n",
    "    plt.bar(x=x - offset, height=hist/len(los)*100, width=width, tick_label=bin_labels)\n",
    "    plt.xlabel('Length of stay (days)')\n",
    "    if show_ylabel:\n",
    "        plt.ylabel('Patients (%)')\n",
    "    plt.xticks(x, rotation=90)\n",
    "    \n",
    "    if los_bg is not None:\n",
    "        hist, bin_edges = np.histogram(los_bg, bins=bins)   \n",
    "        plt.bar(x=x + offset, height=hist/len(los_bg)*100, width=width, color='#AAAAAA')\n",
    "        plt.legend(['cluster', 'full'], loc='best')\n",
    "        \n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "def plot_date_by_month(dates, dates_bg=None, start='2020-03', end='2021-11', show=True):\n",
    "    def get_monthly_stats(dates):\n",
    "        dates = dates.astype('datetime64')\n",
    "        n = len(dates)\n",
    "        # Group by a string: YYYY-MM\n",
    "        counts = dates.groupby(dates.dt.strftime('%Y-%m')).count()\n",
    "        # Make sure months with no patients still show 0 (instead of not showing up in the plot)\n",
    "        counts = counts.reindex(pd.date_range(start, end, freq='MS').strftime(\"%Y-%m\"), fill_value=0)\n",
    "        percent = counts * 100 / n\n",
    "        return pd.DataFrame(percent)\n",
    "        \n",
    "    percent = get_monthly_stats(dates)\n",
    "    \n",
    "    if dates_bg is not None:\n",
    "        percent_bg = get_monthly_stats(dates_bg)\n",
    "        percent_bg.columns = ['full']\n",
    "        percent.columns = ['cluster']\n",
    "        percent = pd.DataFrame(percent).join(percent_bg)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    percent.plot(kind='bar', ax=ax, color=['#1F77B4', '#AAAAAA'])\n",
    "    loc, labels = plt.xticks()\n",
    "    plt.xticks(loc[0:len(loc):2], labels[0:len(labels):2])\n",
    "    plt.ylabel('Patients (%)')\n",
    "    plt.xlabel('Hospitalization month')\n",
    "    ax.get_legend().remove()\n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "def plot_characteristics(df_demographics, df_visit_characteristics, df_demographics_bg=None, df_visit_characteristics_bg=None):\n",
    "    plt.figure(figsize=(12,12))\n",
    "    \n",
    "    plt.subplot(3, 2, 1)\n",
    "    if df_visit_characteristics_bg is not None:\n",
    "        plot_date_by_month(df_visit_characteristics.event_start_date, df_visit_characteristics_bg.event_start_date, show=False)\n",
    "    else:\n",
    "        plot_date_by_month(df_visit_characteristics.event_start_date, show=False)\n",
    "        \n",
    "    plt.subplot(3, 2, 2)\n",
    "    if df_visit_characteristics_bg is not None:\n",
    "        plot_los(df_visit_characteristics.event_visit_los, df_visit_characteristics_bg.event_visit_los, show=False)\n",
    "    else:\n",
    "        plot_los(df_visit_characteristics.event_visit_los, show=False)\n",
    "    \n",
    "    plt.subplot(3, 2, 3)\n",
    "    if df_demographics_bg is not None:\n",
    "        plot_age_histogram(df_demographics.age, df_demographics_bg.age, show=False, skip_xticks=True)\n",
    "    else:\n",
    "        plot_age_histogram(df_demographics.age, show=False, skip_xticks=True)\n",
    "    \n",
    "    plt.subplot(3, 2, 4)\n",
    "    if df_visit_characteristics_bg is not None:\n",
    "        plot_precovid_visit_counts(df_visit_characteristics.visit_count_pre_covid, df_visit_characteristics_bg.visit_count_pre_covid, show=False)\n",
    "    else:\n",
    "        plot_precovid_visit_counts(df_visit_characteristics.visit_count_pre_covid, show=False)\n",
    "    \n",
    "    plt.subplot(3, 2, 5)        \n",
    "    if df_demographics_bg is not None:\n",
    "        plot_race(df_demographics.race_ethnicity, df_demographics_bg.race_ethnicity, show=False, show_ylabel=False)\n",
    "    else:\n",
    "        plot_race(df_demographics.race_ethnicity, show=False, show_ylabel=False)    \n",
    "    \n",
    "    plt.subplot(3, 2, 6)\n",
    "    if df_visit_characteristics_bg is not None:\n",
    "        plot_covid_visit_counts(df_visit_characteristics.visit_count_covid, df_visit_characteristics_bg.visit_count_covid, show=False)\n",
    "    else:\n",
    "        plot_covid_visit_counts(df_visit_characteristics.visit_count_covid, show=False)\n",
    "    plt.show()        \n",
    "    \n",
    "def plot_characteristics_paper(df_demographics, df_visit_characteristics, df_demographics_bg=None, df_visit_characteristics_bg=None, figsize=(12, 7), file_out=None):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    translation = mtransforms.ScaledTranslation(8/72, -15/72, fig.dpi_scale_trans)\n",
    "    \n",
    "    ax = plt.subplot(2, 3, 1)\n",
    "    if df_demographics_bg is not None:\n",
    "        plot_age_histogram(df_demographics.age, df_demographics_bg.age, show=False, skip_xticks=True)\n",
    "    else:\n",
    "        plot_age_histogram(df_demographics.age, show=False, skip_xticks=True)\n",
    "    plt.text(0, 1, '(A)', transform=ax.transAxes + translation, fontweight='bold')\n",
    "        \n",
    "    ax = plt.subplot(2, 3, 2)          \n",
    "    if df_demographics_bg is not None:\n",
    "        plot_race(df_demographics.race_ethnicity, df_demographics_bg.race_ethnicity, show=False, show_ylabel=False)\n",
    "    else:\n",
    "        plot_race(df_demographics.race_ethnicity, show=False, show_ylabel=False)            \n",
    "    plt.text(0, 1, '(B)', transform=ax.transAxes + translation, fontweight='bold')\n",
    "        \n",
    "    ax = plt.subplot(2, 3, 3)\n",
    "    if df_visit_characteristics_bg is not None:\n",
    "        plot_los(df_visit_characteristics.event_visit_los, df_visit_characteristics_bg.event_visit_los, show=False, show_ylabel=False)\n",
    "    else:\n",
    "        plot_los(df_visit_characteristics.event_visit_los, show=False, show_ylabel=False)     \n",
    "    plt.text(0, 1, '(C)', transform=ax.transAxes + translation, fontweight='bold')\n",
    "    \n",
    "    ax = plt.subplot(2, 3, 4)\n",
    "    if df_visit_characteristics_bg is not None:\n",
    "        plot_date_by_month(df_visit_characteristics.event_start_date, df_visit_characteristics_bg.event_start_date, show=False)\n",
    "    else:\n",
    "        plot_date_by_month(df_visit_characteristics.event_start_date, show=False)\n",
    "    plt.text(0.05, 1, '(D)', transform=ax.transAxes + translation, fontweight='bold')\n",
    "    \n",
    "    ax = plt.subplot(2, 3, 5)\n",
    "    if df_visit_characteristics_bg is not None:\n",
    "        plot_precovid_visit_counts(df_visit_characteristics.visit_count_pre_covid, df_visit_characteristics_bg.visit_count_pre_covid, show=False, show_ylabel=False)\n",
    "    else:\n",
    "        plot_precovid_visit_counts(df_visit_characteristics.visit_count_pre_covid, show=False, show_ylabel=False)\n",
    "    plt.text(0.10, 1, '(E)', transform=ax.transAxes + translation, fontweight='bold')\n",
    "    \n",
    "    ax = plt.subplot(2, 3, 6)\n",
    "    if df_visit_characteristics_bg is not None:\n",
    "        plot_covid_visit_counts(df_visit_characteristics.visit_count_covid, df_visit_characteristics_bg.visit_count_covid, show=False, show_ylabel=False)\n",
    "    else:\n",
    "        plot_covid_visit_counts(df_visit_characteristics.visit_count_covid, show=False, show_ylabel=False)\n",
    "    plt.text(0.13, 1, '(F)', transform=ax.transAxes + translation, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if file_out is not None:\n",
    "        plt.savefig(file_out)        \n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entire Dataset Characteristics (including held-out data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics_orig = demographics(df_events_orig)\n",
    "df_visit_characteristics_orig = visit_characteristics(df_events_orig, time_window_pre, time_window_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_ethnicity_orig = df_demographics_orig.race_ethnicity.value_counts()\n",
    "race_ethnicity_orig = race_ethnicity_orig.reindex(index=['Hispanic', 'Asian, AIAN, or NHPI', 'Black or African American', 'White', 'Other or Unknown'])\n",
    "display(race_ethnicity_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal layout of plots\n",
    "plot_characteristics_paper(df_demographics_orig, df_visit_characteristics_orig, figsize=(10, 7), \n",
    "                           file_out=path.join(dir_output, 'full_cohort_distributions.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex = df_demographics_orig.sex.value_counts()\n",
    "sex = sex / np.sum(sex) * 100\n",
    "display(sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-COVID number of visits - print percentages\n",
    "print('pre-COVID')\n",
    "hist, bin_edges = plot_precovid_visit_counts(df_visit_characteristics_orig.visit_count_pre_covid, show=False)\n",
    "for i, j in zip(hist, bin_edges):\n",
    "    print(f'{i / n_patients_orig * 100:.01f}: {j}')\n",
    "    \n",
    "# COVID number of visits - print percentages\n",
    "print('\\nCOVID')\n",
    "hist, bin_edges = plot_covid_visit_counts(df_visit_characteristics_orig.visit_count_covid, show=False)\n",
    "for i, j in zip(hist, bin_edges):\n",
    "    print(f'{i / n_patients_orig * 100:.01f}: {j}')\n",
    "    \n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaccination rate\n",
    "vax_full = vaccination_rate(df_events_orig, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID Severity Phenotyping\n",
    "class COVIDSeverity:    \n",
    "    MODERATE = 1\n",
    "    SEVERE = 2\n",
    "    CRITICAL = 3    \n",
    "    \n",
    "def cps_severity(cps):\n",
    "    concepts_critical = [\n",
    "        # Conditions\n",
    "        434489,   # Dead\n",
    "        4195694,  # ARDS\n",
    "        46273390, # Dependence on respirator\n",
    "        196236,   # Septic shock\n",
    "        # Procedures\n",
    "        2106469, # Intubation, endotracheal, emergency procedure\n",
    "        2745444, # Insertion of Endotracheal Airway into Trachea, Via Natural or Artificial Opening    \n",
    "        2788036, # Respiratory Ventilation, Less than 24 Consecutive Hours\n",
    "        2788037, # Respiratory Ventilation, 24-96 Consecutive Hours\n",
    "        2788038, # Respiratory Ventilation, Greater than 96 Consecutive Hours\n",
    "    ]\n",
    "    concepts_severe = [\n",
    "        # Conditions\n",
    "        46271075, # AHRF\n",
    "        437390,   # Hypoxemia\n",
    "        # Procedures - non-invasive respiratory ventilation\n",
    "        1781160,\n",
    "        2788018,\n",
    "        2788017,\n",
    "        2788016,\n",
    "        2787824,\n",
    "        2787823,\n",
    "        2788019,\n",
    "        2788020,\n",
    "        2788021,\n",
    "        1781161,\n",
    "        2788022,\n",
    "        2788023,\n",
    "        2788024,\n",
    "        2788025,\n",
    "        2788026,\n",
    "        1781162,\n",
    "        2788027,\n",
    "        2788028,\n",
    "    ]\n",
    "    for c in concepts_critical:\n",
    "        if str(c) in cps.sequence.words:\n",
    "            return COVIDSeverity.CRITICAL\n",
    "    for c in concepts_severe:\n",
    "        if str(c) in cps.sequence.words:\n",
    "            return COVIDSeverity.SEVERE\n",
    "    return COVIDSeverity.MODERATE\n",
    "\n",
    "SeverityTuple = namedtuple('SeverityTuple', ['Moderate', 'Severe', 'Critical'])\n",
    "\n",
    "patient_severity_orig = dict()\n",
    "for pid, cps in cpss_orig.items():\n",
    "    patient_severity_orig[pid] = cps_severity(cps)\n",
    "\n",
    "severity_list = np.array(list(patient_severity_orig.values()))    \n",
    "severity_orig_cnt = SeverityTuple(np.sum(severity_list == COVIDSeverity.MODERATE), np.sum(severity_list == COVIDSeverity.SEVERE), np.sum(severity_list == COVIDSeverity.CRITICAL))\n",
    "severity_orig_pct = SeverityTuple(*[c / n_patients_orig * 100 for c in severity_orig_cnt])\n",
    "print(f'Moderate COVID: {severity_orig_cnt.Moderate} / {n_patients_orig} ({severity_orig_pct.Moderate:.01f}%)')\n",
    "print(f'Severe COVID: {severity_orig_cnt.Severe} / {n_patients_orig} ({severity_orig_pct.Severe:.01f}%)')\n",
    "print(f'Critical COVID: {severity_orig_cnt.Critical} / {n_patients_orig} ({severity_orig_pct.Critical:.01f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create patient vectors for cohort   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times to run inference on a patient sequence. Mean of inferred vectors will be used\n",
    "n_inferences = 1\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "# Store the patient vectors in a list. The list should have the same order as cpss\n",
    "patient_vectors_orig = list()\n",
    "patient_vectors_dict_orig = dict()\n",
    "for i, pid in enumerate(cpss_orig):\n",
    "    cps = cpss_orig[pid]    \n",
    "    l_dm = list()\n",
    "    l_dbow = list()\n",
    "    \n",
    "    # Take the mean of n_inferences iterations of inferring the vector\n",
    "    for _ in range(n_inferences):\n",
    "        l_dm.append(model_dm.infer_vector(cps.sequence.words))\n",
    "        l_dbow.append(model_dbow.infer_vector(cps.sequence.words))\n",
    "    a_dm = np.mean(np.array(l_dm), axis=0)\n",
    "    a_dbow = np.mean(np.array(l_dbow), axis=0)\n",
    "    \n",
    "    # Concatenate DM and DBOW to form a single vector\n",
    "    patient_vector = np.concatenate((a_dm, a_dbow))  \n",
    "    \n",
    "    # Store\n",
    "    patient_vectors_orig.append(patient_vector)\n",
    "    patient_vectors_dict_orig[pid] = patient_vector\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        ellapsed_time = (time() - t1) / 60\n",
    "        print(f'{i}: {ellapsed_time:.02f} min')\n",
    "        \n",
    "# Convert to numpy array\n",
    "patient_vectors_orig = np.array(patient_vectors_orig)\n",
    "\n",
    "file_vectors = path.join(dir_output, 'covid_cohort_patient_vectors_orig.pkl')\n",
    "with open(file_vectors, 'wb') as f:\n",
    "    pickle.dump(patient_vectors_dict_orig, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hold out test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_holdout = round(n_patients_orig/10)\n",
    "print(n_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-time hold out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_holdout_oot = df_events_orig.iloc[-n_holdout:, :]\n",
    "df_events = df_events_orig.iloc[:-n_holdout, :]\n",
    "patient_ids_holdout_oot = df_events_holdout_oot.index.tolist()\n",
    "print(len(df_events_holdout_oot))\n",
    "print(len(df_events))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-time hold out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Random(42)\n",
    "patient_ids_holdout_it = r.sample(df_events.index.tolist(), n_holdout)\n",
    "df_events_holdout_it = df_events.loc[patient_ids_holdout_it, :].sort_values(by='start_date')\n",
    "df_events = df_events[~df_events.index.isin(patient_ids_holdout_it)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up the training vs holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined holdout \n",
    "patient_ids_holdout = patient_ids_holdout_oot + patient_ids_holdout_it\n",
    "\n",
    "# Data to train on\n",
    "patient_ids = list(set(patient_ids_orig) - set(patient_ids_holdout))\n",
    "n_patients = len(patient_ids)\n",
    "cpss_pre = dict()\n",
    "cpss = dict()\n",
    "mask_train = np.zeros(n_patients_orig, dtype=np.bool_)\n",
    "for i, pid in enumerate(cpss_orig.keys()):\n",
    "    if pid in patient_ids:\n",
    "        cpss_pre[pid] = cpss_pre_orig[pid]\n",
    "        cpss[pid] = cpss_orig[pid]\n",
    "        mask_train[i] = True        \n",
    "patient_vectors = patient_vectors_orig[mask_train, :]\n",
    "patient_ids = list(cpss.keys())  # keep patient_ids and patient_vectors in parallel\n",
    "print(len(cpss_pre))\n",
    "print(len(cpss))\n",
    "print(patient_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demographics = demographics(df_events)\n",
    "df_visit_characteristics = visit_characteristics(df_events, time_window_pre, time_window_covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_ethnicity_full = df_demographics.race_ethnicity.value_counts()\n",
    "race_ethnicity_full = race_ethnicity_full.reindex(index=['Hispanic', 'Asian, AIAN, or NHPI', 'Black or African American', 'White', 'Other or Unknown'])\n",
    "display(race_ethnicity_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_characteristics_paper(df_demographics, df_visit_characteristics, figsize=(10, 7), \n",
    "                           file_out=path.join(dir_output, 'full_cohort_distributions_training.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaccination rate\n",
    "vax_full = vaccination_rate(df_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_severity = dict()\n",
    "for pid, cps in cpss.items():\n",
    "    patient_severity[pid] = cps_severity(cps)\n",
    "\n",
    "severity_list = np.array(list(patient_severity.values()))    \n",
    "severity_full_cnt = SeverityTuple(np.sum(severity_list == COVIDSeverity.MODERATE), np.sum(severity_list == COVIDSeverity.SEVERE), np.sum(severity_list == COVIDSeverity.CRITICAL))\n",
    "severity_full_pct = SeverityTuple(*[c / n_patients * 100 for c in severity_full_cnt])\n",
    "print(f'Moderate COVID: {severity_full_cnt.Moderate} / {n_patients} ({severity_full_pct.Moderate:.01f}%)')\n",
    "print(f'Severe COVID: {severity_full_cnt.Severe} / {n_patients} ({severity_full_pct.Severe:.01f}%)')\n",
    "print(f'Critical COVID: {severity_full_cnt.Critical} / {n_patients} ({severity_full_pct.Critical:.01f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE using the defined disease subtypes  \n",
    "this takes about 75 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time()\n",
    "\n",
    "tsne = TSNE(n_components=2, metric='euclidean', perplexity=30, learning_rate=1000, init='pca', \n",
    "            n_iter=1000, n_jobs=8, verbose=2, random_state=42)\n",
    "patient_vectors_tsne = tsne.fit_transform(patient_vectors)\n",
    "\n",
    "ellapsed_time = (time() - t1) / 60\n",
    "print(f'{ellapsed_time} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE plot\n",
    "plt.figure(figsize=(16,12))\n",
    "known_labels = [df_concepts.loc[cps.label, 'concept_name'] for cps in cpss.values()]\n",
    "sns.scatterplot(patient_vectors_tsne[:,0], patient_vectors_tsne[:,1], \n",
    "                hue=known_labels, legend='full', palette=sns.color_palette('bright', 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow method to select K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distortion for a range of number of cluster\n",
    "distortions = []\n",
    "kmeans_dict = dict()\n",
    "r = range(1, 60)\n",
    "for i in r:\n",
    "    km = KMeans(n_clusters=i, random_state=42, n_jobs=8)\n",
    "    kmeans_dict[i] = km.fit(patient_vectors)\n",
    "    distortions.append(km.inertia_)\n",
    "    \n",
    "    # Progress\n",
    "    if (i >= 30) and (i % 5 == 0):\n",
    "        print(i)\n",
    "    \n",
    "# Save the kmeans models\n",
    "f_kmeans = f'kmeans_{datetime.now().isoformat()}.pkl'\n",
    "f_kmeans = path.join(dir_output, f_kmeans)\n",
    "pickle.dump(kmeans_dict, open(f_kmeans, 'wb'))      \n",
    "\n",
    "# plot\n",
    "plt.plot(r, distortions, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ks_plot = [10, 15, 20, 25, 30]\n",
    "for k in ks_plot:\n",
    "    clustering = kmeans_dict[k]\n",
    "    predicted_labels = clustering.predict(patient_vectors)    \n",
    "    palette_kmeans = sns.color_palette('bright', k)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.scatterplot(patient_vectors_tsne[:,0], patient_vectors_tsne[:,1], \n",
    "                    hue=predicted_labels, legend='full', palette=palette_kmeans,\n",
    "                    style=predicted_labels, markers=['o']*10+['X']*10+['v']*10+['d']*10+['s']*10+['P']*10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the desired model\n",
    "n_clusters = 20\n",
    "clustering = kmeans_dict[n_clusters]\n",
    "predicted_labels = clustering.predict(patient_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Cohort Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of patient sequences for each known subtype\n",
    "patients_known_disease_subtype = defaultdict(list)\n",
    "for cps in cpss.values():\n",
    "    patients_known_disease_subtype[cps.label].append(cps)\n",
    "\n",
    "# dict[disease subtype concept_id] => dict[domain_id] => dict[concept_id] => Counter\n",
    "known_disease_profiles = dict()\n",
    "\n",
    "# For each disease subtype, count the number of patients each concept is observed in\n",
    "for known_disease_subtype, cohort_cpss in patients_known_disease_subtype.items():\n",
    "    # dict[domain_id] => Counter\n",
    "    cohort_domain_concept_counter = defaultdict(Counter)\n",
    "    \n",
    "    for cohort_cps in cohort_cpss:\n",
    "        # cohort_cps.sequence is a TaggedDoc with sequence of concepts stored as strings. \n",
    "        # Convert it to a list of concept_ids (ints)\n",
    "        seq = [int(x) for x in cohort_cps.sequence.words]\n",
    "        \n",
    "        # Keep track of which conpepts we've already seen for this patient so we don't add again\n",
    "        concepts_observed = list()  \n",
    "        \n",
    "        for concept in seq:\n",
    "            if concept in concepts_observed:\n",
    "                # Already seen this concept for this patient\n",
    "                continue            \n",
    "                \n",
    "            domain = df_concepts.loc[concept, 'domain_id']\n",
    "            cohort_domain_concept_counter[domain][concept] += 1\n",
    "            concepts_observed.append(concept)\n",
    "            \n",
    "    known_disease_profiles[known_disease_subtype] = cohort_domain_concept_counter    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of patient sequences for each known CKD subtype\n",
    "patients_known_disease_subtype_pre = defaultdict(list)\n",
    "for cps in cpss_pre.values():\n",
    "    patients_known_disease_subtype_pre[cps.label].append(cps)\n",
    "\n",
    "# dict[disease subtype concept_id] => dict[domain_id] => dict[concept_id] => Counter\n",
    "known_disease_profiles_pre = dict()\n",
    "\n",
    "# For each disease subtype, count the number of patients each concept is observed in\n",
    "for known_disease_subtype, cohort_cpss in patients_known_disease_subtype_pre.items():\n",
    "    # dict[domain_id] => Counter\n",
    "    cohort_domain_concept_counter = defaultdict(Counter)\n",
    "    \n",
    "    for cohort_cps in cohort_cpss:\n",
    "        # cohort_cps.sequence is a TaggedDoc with sequence of concepts stored as strings. \n",
    "        # Convert it to a list of concept_ids (ints)\n",
    "        seq = [int(x) for x in cohort_cps.sequence.words]\n",
    "        \n",
    "        # Keep track of which conpepts we've already seen for this patient so we don't add again\n",
    "        concepts_observed = list()  \n",
    "        \n",
    "        for concept in seq:\n",
    "            if concept in concepts_observed:\n",
    "                # Already seen this concept for this patient\n",
    "                continue            \n",
    "                \n",
    "            domain = df_concepts.loc[concept, 'domain_id']\n",
    "            cohort_domain_concept_counter[domain][concept] += 1\n",
    "            concepts_observed.append(concept)\n",
    "            \n",
    "    known_disease_profiles_pre[known_disease_subtype] = cohort_domain_concept_counter    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned Disease Subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtype_profiling(cpss, predicted_labels):\n",
    "    # Build a list of patient sequences for each predicted (learned) disease subtype\n",
    "    patients_learned_disease_subtype = defaultdict(list)\n",
    "    for i, cps in enumerate(cpss.values()):\n",
    "        patients_learned_disease_subtype[predicted_labels[i]].append(cps)\n",
    "\n",
    "    # dict[disease subtype concept_id] => dict[domain_id] => dict[concept_id] => Counter\n",
    "    learned_disease_profiles = dict()\n",
    "\n",
    "    # For each disease subtype, count the number of patients each concept is observed in\n",
    "    for disease_subtype, cohort_cpss in patients_learned_disease_subtype.items():\n",
    "        # dict[domain_id] => Counter\n",
    "        cohort_domain_concept_counter = defaultdict(Counter)\n",
    "\n",
    "        for cohort_cps in cohort_cpss:\n",
    "            # cohort_cps.sequence is a TaggedDoc with sequence of concepts stored as strings. \n",
    "            # Convert it to a list of concept_ids (ints)\n",
    "            seq = [int(x) for x in cohort_cps.sequence.words]\n",
    "\n",
    "            # Keep track of which conpepts we've already seen for this patient so we don't add again\n",
    "            concepts_observed = list()  \n",
    "\n",
    "            for concept in seq:\n",
    "                if concept in concepts_observed:\n",
    "                    # Already seen this concept for this patient\n",
    "                    continue            \n",
    "\n",
    "                domain = df_concepts.loc[concept, 'domain_id']\n",
    "                cohort_domain_concept_counter[domain][concept] += 1\n",
    "                concepts_observed.append(concept)\n",
    "\n",
    "        learned_disease_profiles[disease_subtype] = cohort_domain_concept_counter    \n",
    "        \n",
    "    return patients_learned_disease_subtype, learned_disease_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographics = namedtuple('Demographics', ['age', 'sex'])\n",
    "\n",
    "def subtype_demographics(patients_learned_disease_subtype, df_demographics):\n",
    "    subtype_demogs = dict()\n",
    "    for disease_subtype, cohort_cpss in patients_learned_disease_subtype.items():\n",
    "        person_ids = [x.person_id for x in cohort_cpss]\n",
    "        subtype_demogs[disease_subtype] = df_demographics.loc[df_demographics.index.isin(person_ids), :]\n",
    "    return subtype_demogs                \n",
    "\n",
    "def subtype_visit_profiles(patients_learned_disease_subtype, df_visit_characteristics):\n",
    "    subtype_visit_profs = dict()\n",
    "    for disease_subtype, cohort_cpss in patients_learned_disease_subtype.items():\n",
    "        person_ids = [x.person_id for x in cohort_cpss]\n",
    "        subtype_visit_profs[disease_subtype] = df_visit_characteristics.loc[df_visit_characteristics.index.isin(person_ids), :]\n",
    "    return subtype_visit_profs\n",
    "\n",
    "def subtype_primary_discharge_diagnoses(patients_learned_disease_subtype, df_events):\n",
    "    subtype_discharge_diagnoses = dict()\n",
    "    for disease_subtype, cohort_cpss in patients_learned_disease_subtype.items():\n",
    "        person_ids = [x.person_id for x in cohort_cpss]\n",
    "        df_events_subtype = df_events.loc[df_events.index.isin(person_ids), :]\n",
    "        visit_occurrence_ids = [str(x) for x in df_events_subtype.visit_occurrence_id.tolist()]\n",
    "        \n",
    "        # Reduces the weight of a PDD if a patient has multiple PDDs\n",
    "        sql = f'''WITH pdd AS (SELECT person_id, co.condition_concept_id\n",
    "        FROM condition_occurrence co\n",
    "        WHERE co.condition_status_concept_id = 32903 -- primary discharge diagnosis\n",
    "            AND visit_occurrence_id IN ({','.join(visit_occurrence_ids)})\n",
    "        ),\n",
    "\n",
    "        pc AS (SELECT person_id, 1.0/count(*) AS weight FROM pdd GROUP BY person_id)\n",
    "\n",
    "        SELECT c.concept_id, c.concept_name, sum(weight) AS count\n",
    "        FROM pdd\n",
    "        JOIN pc ON pdd.person_id = pc.person_id \n",
    "        JOIN concept c ON pdd.condition_concept_id = c.concept_id\n",
    "        GROUP BY c.concept_id, c.concept_name\n",
    "        ORDER BY count DESC;\n",
    "        '''\n",
    "\n",
    "        df_discharge_diag = pd.read_sql(sql, engine)\n",
    "        n_persons = len(person_ids)\n",
    "        df_discharge_diag['percent'] = df_discharge_diag['count'] / n_persons * 100\n",
    "        subtype_discharge_diagnoses[disease_subtype] = df_discharge_diag\n",
    "    return subtype_discharge_diagnoses\n",
    "\n",
    "def subtype_vaccination(patients_learned_disease_subtype, df_events):\n",
    "    vaccination = dict()\n",
    "    for disease_subtype, cohort_cpss in patients_learned_disease_subtype.items():\n",
    "        person_ids = [x.person_id for x in cohort_cpss]\n",
    "        df_events_subtype = df_events[df_events.index.isin(person_ids)]\n",
    "        vaccination[disease_subtype] = vaccination_rate(df_events_subtype)\n",
    "    return vaccination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_learned_disease_subtype, learned_disease_profiles = subtype_profiling(cpss, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_learned_disease_subtype_pre, learned_disease_profiles_pre = subtype_profiling(cpss_pre, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtype_demogs = subtype_demographics(patients_learned_disease_subtype, df_demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtype_visit_profs = subtype_visit_profiles(patients_learned_disease_subtype, df_visit_characteristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtype_discharge_diagnoses = subtype_primary_discharge_diagnoses(patients_learned_disease_subtype, df_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtype_vaccinations = subtype_vaccination(patients_learned_disease_subtype, df_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtype_severe_covid(patients_learned_disease_subtype):\n",
    "    severity_cnts = dict()\n",
    "    severity_pcts = dict()\n",
    "    for disease_subtype, cohort_cpss in patients_learned_disease_subtype.items():                \n",
    "        severities = np.array([patient_severity[cps.person_id] for cps in cohort_cpss])\n",
    "        n_subtype = len(cohort_cpss)\n",
    "        n_moderate = np.sum(severities == COVIDSeverity.MODERATE)\n",
    "        n_severe = np.sum(severities == COVIDSeverity.SEVERE)\n",
    "        n_critical = np.sum(severities == COVIDSeverity.CRITICAL)\n",
    "        severity_cnt = SeverityTuple(n_moderate, n_severe, n_critical)\n",
    "        severity_cnts[disease_subtype] = severity_cnt\n",
    "        severity_pcts[disease_subtype] = SeverityTuple(*[n / n_subtype * 100 for n in severity_cnt])\n",
    "    return severity_cnts, severity_pcts\n",
    "\n",
    "subtype_severity_cnts, subtype_severity_pcts = subtype_severe_covid(patients_learned_disease_subtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-arrange groups labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def severity_score(severity_pct_tuple):\n",
    "    return severity_pct_tuple.Moderate*1 + severity_pct_tuple.Severe*2 + severity_pct_tuple.Critical*3\n",
    "\n",
    "# Sort the subtypes by severity phenotype\n",
    "# severity_scores = [subtype_severity_pcts[i].Moderate/100*1 + subtype_severity_pcts[i].Severe/100*2 + subtype_severity_pcts[i].Critical/100*3 for i in range(n_clusters)]\n",
    "severity_scores = [severity_score(subtype_severity_pcts[i]) for i in range(n_clusters)]\n",
    "subtype_severity_index_to_cluster_index = {i+1:x for i,x in enumerate(np.argsort(severity_scores, axis=0))}\n",
    "subtype_cluster_index_to_severity_index = {v:k for k,v in subtype_severity_index_to_cluster_index.items()}\n",
    "predicted_labels_renamed = [subtype_cluster_index_to_severity_index[i] for i in predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtype_cluster_index_to_custom_index = subtype_cluster_index_to_severity_index\n",
    "subtype_custom_index_to_cluster_index = subtype_severity_index_to_cluster_index\n",
    "predicted_labels_renamed = [subtype_cluster_index_to_custom_index[i] for i in predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the t-SNE with new labels (this was generated with LOS median, 75%, 25%)\n",
    "palette_kmeans = sns.color_palette('bright', 10)\n",
    "# Color spectrum order\n",
    "palette_kmeans = [palette_kmeans[i] for i in [3, 1, 8, 2, 9, 0, 4, 6, 5, 7, 3, 1, 8, 2, 9, 0, 4, 6, 5, 7]]  \n",
    "plt.figure(figsize=(13,8))\n",
    "sns.scatterplot(patient_vectors_tsne[:,0], patient_vectors_tsne[:,1], \n",
    "                hue=predicted_labels_renamed, legend='full', palette=palette_kmeans,\n",
    "                style=predicted_labels_renamed, markers=['o']*10+['X']*10+['v']*10+['d']*10+['s']*10+['P']*10,\n",
    "                alpha=0.6)  \n",
    "plt.savefig(path.join(dir_output, 'tsne.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_style(sig, reverse=False):\n",
    "    sty = 'color: black'\n",
    "    if reverse:\n",
    "        sig = sig * -1\n",
    "    if sig < 0:\n",
    "        sty = 'font-weight: bold; color: blue'\n",
    "    elif sig > 0:\n",
    "        sty = 'font-weight: bold; color: red'\n",
    "    return sty\n",
    "\n",
    "def sig_style_binary(sig):    \n",
    "    if sig:\n",
    "        return 'font-weight: bold; color: black'\n",
    "    else:\n",
    "        return 'color: black'\n",
    "\n",
    "def _chi(x_subtype, n_disease_subtype, x_full, n_patients, alpha, complement=True):\n",
    "    obs_sub = np.array([x_subtype, n_disease_subtype - x_subtype])\n",
    "    obs_comp = np.array([x_full, n_patients - x_full])\n",
    "    if complement:\n",
    "        obs_comp -= obs_sub\n",
    "    obs = np.array([obs_sub, obs_comp])\n",
    "    _, p, _, _ = chi2_contingency(obs)\n",
    "    return p < alpha\n",
    "\n",
    "def _chi_str(x_subtype, n_disease_subtype, x_full, n_patients, alpha, complement=True):\n",
    "    sig = _chi(x_subtype, n_disease_subtype, x_full, n_patients, alpha, complement)\n",
    "    return '*' if sig else ''\n",
    "\n",
    "def _chi_style(x_subtype, n_disease_subtype, x_full, n_patients, alpha, complement=True):\n",
    "    sig = _chi(x_subtype, n_disease_subtype, x_full, n_patients, alpha, complement)\n",
    "    p_sub = x_subtype / n_disease_subtype\n",
    "    p_full = x_full / n_patients\n",
    "    return sig_style(sig * 1 if p_sub > p_full else -1)\n",
    "    \n",
    "\n",
    "def _chi_freq(f_subtype, f_full, alpha, complement=True):\n",
    "    if complement:\n",
    "        f_full = f_full - f_subtype\n",
    "    obs = np.array([f_subtype, f_full])\n",
    "    _, p, _, _ = chi2_contingency(obs)\n",
    "    return p < alpha\n",
    "\n",
    "def _chi_freq_style(f_subtype, f_full, alpha, complement=True):\n",
    "    sig = _chi_freq(f_subtype, f_full, alpha, complement)\n",
    "    return sig_style_binary(sig)\n",
    "\n",
    "def _med_iqr(x, ref=None, alpha=None, complement=True):\n",
    "    q = np.percentile(x, [50, 25, 75])\n",
    "    s1 = f'{q[0]:.01f}' \n",
    "    s2 = f'[{q[1]:.01f}, {q[2]:.01f}]'\n",
    "    sig = 0\n",
    "    if ref is not None and alpha is not None:\n",
    "        if complement:\n",
    "            # Assume ref contains x in it. Remove each instance of x\n",
    "            ref = list(ref)\n",
    "            for xi in x:\n",
    "                ref.remove(xi)\n",
    "        \n",
    "        # Mann-Whitney U Test against ref\n",
    "        p = mannwhitneyu(x, ref)[1]\n",
    "        if p < alpha:\n",
    "            ref_med = np.percentile(ref, [50])\n",
    "            if q[0] < ref_med[0]:\n",
    "                sig = -1\n",
    "            elif q[0] > ref_med[0]:\n",
    "                sig = 1\n",
    "            else:\n",
    "                # medians are the same. compare means to differentiate\n",
    "                sig = -1 if np.mean(x) < np.mean(ref) else 1\n",
    "                    \n",
    "    return s1, s2, sig\n",
    "\n",
    "def _med_iqr_h(x, ref=None, alpha=None, complement=True):\n",
    "    s1, s2, sig = _med_iqr(x, ref, alpha, complement)\n",
    "    s = s1 + ' ' + s2\n",
    "    if sig:\n",
    "        s += '*'\n",
    "    return s\n",
    "\n",
    "def _med_iqr_v(x, ref=None, alpha=None, style=False, complement=True):\n",
    "    s1, s2, sig = _med_iqr(x, ref, alpha, complement)\n",
    "    if style:\n",
    "        return s1 + '\\n' + s2, sig_style(sig)\n",
    "    else:\n",
    "        return s1 + ('*' if sig else '') + '\\n' + s2\n",
    "\n",
    "def _med_iqr_dates(x, ref=None, alpha=None, complement=True):\n",
    "    x = x.astype('datetime64[ns]')\n",
    "    q = x.quantile([0.5, 0.25, 0.75], interpolation=\"linear\")\n",
    "    s1 = f'{str(q[0.5])[:10]}' \n",
    "    s2 = f'[{str(q[0.25])[:10]},'\n",
    "    s3 = f'{str(q[0.75])[:10]}]'\n",
    "    sig = 0\n",
    "    if ref is not None and alpha is not None:\n",
    "        ref = ref.astype('datetime64[ns]')\n",
    "        ref_med = ref.quantile([0.5], interpolation='linear')\n",
    "        ref = list(ref)\n",
    "        if complement:\n",
    "            # Assume ref contains x in it. Remove each instance of x\n",
    "            ref = list(ref)\n",
    "            for xi in x:\n",
    "                ref.remove(xi)\n",
    "                \n",
    "        # Mann-Whitney U Test against ref\n",
    "        p = mannwhitneyu(list(x), ref)[1]\n",
    "        if p < alpha:\n",
    "            \n",
    "            if q[0.5] < ref_med[0.5]:\n",
    "                sig = -1\n",
    "            else:\n",
    "                sig = 1            \n",
    "    return s1, s2, s3, sig\n",
    "\n",
    "    \n",
    "def _med_iqr_dates_h(x, ref=None, alpha=None, style=False):\n",
    "    s1, s2, s3, sig = _med_iqr_dates(x, ref, alpha)\n",
    "    s = ' '.join([s1, s2, s3])\n",
    "    if style:\n",
    "        return s, sig_style(sig)\n",
    "    else:\n",
    "        if sig:\n",
    "            s += '*'\n",
    "        return s    \n",
    "\n",
    "def _med_iqr_dates_v(x, ref=None, alpha=None, style=False):\n",
    "    s1, s2, s3, sig = _med_iqr_dates(x, ref, alpha)\n",
    "    if style:\n",
    "        return '\\n'.join([s1, s2, s3]), sig_style(sig)\n",
    "    else:\n",
    "        if sig:\n",
    "            s1 += '*'\n",
    "        return '\\n'.join([s1, s2, s3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_count = 'Count'\n",
    "name_age = 'Age'\n",
    "name_sex = 'Sex (female)'\n",
    "name_eth_race = '\\n'.join(['Race-ethnicity'] + list(race_ethnicity_full.index))\n",
    "name_vax = 'Vaccinated (%)'\n",
    "name_start = 'Hospital start date'\n",
    "name_los = 'Hospital length (days)'\n",
    "name_visits_covid = 'Number of visits'\n",
    "name_visits_pre = 'Number of prior visits'\n",
    "names = [name_count, name_age, name_sex, name_eth_race, name_vax, name_start, name_los, name_visits_covid, name_visits_pre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a summary statistics table \n",
    "df_summary = pd.DataFrame(data=None, index=names, columns=['Full']+[x for x in range(1, n_clusters+1)])\n",
    "df_style = pd.DataFrame(data='color: black', index=names, columns=['Full']+[x for x in range(1, n_clusters+1)])\n",
    "\n",
    "# Full COVID cohort\n",
    "s_count = f'{n_patients} (100%)'\n",
    "s_age = _med_iqr_v(df_demographics.age)\n",
    "n_female_full = np.sum(df_demographics.sex == \"FEMALE\")\n",
    "p_sex_full = n_female_full / n_patients * 100\n",
    "s_sex = f'{n_female_full} ({p_sex_full:.01f}%)'\n",
    "s_race_ethnicity = '\\n'.join([''] + [f'{p} ({p / n_patients * 100:.01f}%)' for p in race_ethnicity_full])\n",
    "n_vax_full = round(vax_full * n_patients / 100)\n",
    "s_vaccinated = f'{n_vax_full} ({vax_full:.01f}%)' \n",
    "s_visit_start_date = _med_iqr_dates_v(df_visit_characteristics.event_start_date)\n",
    "s_visit_length = _med_iqr_v(df_visit_characteristics.event_visit_los)\n",
    "s_num_visits_covid = _med_iqr_v(df_visit_characteristics.visit_count_covid)\n",
    "s_num_visits_pre_covid = _med_iqr_v(df_visit_characteristics.visit_count_pre_covid)\n",
    "df_summary.loc[:, 'Full'] = [s_count, s_age, s_sex, s_race_ethnicity, s_vaccinated, s_visit_start_date, s_visit_length, s_num_visits_covid, s_num_visits_pre_covid]\n",
    "\n",
    "# Subtypes\n",
    "sty = 'color: black;'\n",
    "alpha = 0.05 / 1000\n",
    "for disease_subtype in range(1, n_clusters+1):\n",
    "    cluster_index = subtype_custom_index_to_cluster_index[disease_subtype]\n",
    "    subtype_demog = subtype_demogs[cluster_index]\n",
    "    subtype_visit_prof =  subtype_visit_profs[cluster_index]\n",
    "    patients_subtype = patients_learned_disease_subtype[cluster_index]\n",
    "    \n",
    "    n_disease_subtype = len(patients_subtype)\n",
    "    s_count = f'{n_disease_subtype} ({n_disease_subtype / n_patients * 100:.01f}%)'\n",
    "    s_age, sty_age = _med_iqr_v(subtype_demog.age, ref=df_demographics.age, alpha=alpha, style=True)    \n",
    "    n_female = np.sum(subtype_demog.sex == \"FEMALE\")\n",
    "    s_sex = f'{n_female} ({(n_female / n_disease_subtype * 100):.01f}%)' \n",
    "    sty_sex = _chi_style(n_female, n_disease_subtype, n_female_full, n_patients, alpha=alpha)    \n",
    "    race_ethnicity = subtype_demog.race_ethnicity.value_counts().reindex(index=race_ethnicity_full.index, fill_value=0)    \n",
    "    s_race_ethnicity = '\\n'.join([''] + [f'{p} ({p / n_disease_subtype * 100:.01f}%)' for p in race_ethnicity])    \n",
    "    sty_race_ethnicity = _chi_freq_style(race_ethnicity, race_ethnicity_full, alpha)\n",
    "    p_vax = subtype_vaccinations[cluster_index]\n",
    "    n_vax = round(p_vax * n_disease_subtype / 100)\n",
    "    s_vaccinated = f'{n_vax} ({p_vax:.01f}%)'\n",
    "    sty_vaccinated = _chi_style(n_vax, n_disease_subtype, n_vax_full, n_patients, alpha=alpha)\n",
    "    s_visit_start_date, sty_visit_start_date = _med_iqr_dates_v(subtype_visit_prof.event_start_date, ref=df_visit_characteristics.event_start_date, alpha=alpha, style=True)\n",
    "    s_visit_length, sty_visit_length = _med_iqr_v(subtype_visit_prof.event_visit_los, ref=df_visit_characteristics.event_visit_los, alpha=alpha, style=True)    \n",
    "    s_num_visits_covid, sty_num_visits_covid = _med_iqr_v(subtype_visit_prof.visit_count_covid, ref=df_visit_characteristics.visit_count_covid, alpha=alpha, style=True)\n",
    "    s_num_visits_pre_covid, sty_num_visits_pre_covid = _med_iqr_v(subtype_visit_prof.visit_count_pre_covid, ref=df_visit_characteristics.visit_count_pre_covid, alpha=alpha, style=True)    \n",
    "    df_summary.loc[:, disease_subtype] = [s_count, s_age, s_sex, s_race_ethnicity, s_vaccinated, s_visit_start_date, s_visit_length, s_num_visits_covid, s_num_visits_pre_covid]\n",
    "    df_style.loc[:, disease_subtype] = [sty, sty_age, sty_sex, sty_race_ethnicity, sty_vaccinated, sty_visit_start_date, sty_visit_length, sty_num_visits_covid, sty_num_visits_pre_covid]\n",
    "\n",
    "df_styler_summary_h = df_summary.style.apply(lambda _: df_style, axis=None).applymap(lambda _: 'white-space: pre-wrap').applymap_index(lambda _: 'white-space: pre-wrap')\n",
    "display(df_styler_summary_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2. COVID-19 subgroup concept prevalences using concept sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_sets = [\n",
    "    # ['COVID-19', [37311061]],\n",
    "    ['Fever', [437663]],\n",
    "    ['Cough', [254761]],\n",
    "    ['Diarrhea', [196523]],\n",
    "    ['Constipation', [75860]],\n",
    "    ['Dyspnea', [312437]],\n",
    "    ['Viral pneumonia', [261326, 3661408]],\n",
    "    ['Hypoxemia', [437390]],\n",
    "    ['AHRF', [46271075, 37395564]],\n",
    "    ['ARDS', [4195694, 4191650]],\n",
    "    ['Pleural effusion', [254061]],\n",
    "    ['Atelectasis', [261880]],\n",
    "    ['Anemia', [439777]],\n",
    "    ['Tachycardia', [444070, 4275423, 4103295]],\n",
    "    ['Heart failure', [319835, 316139, 40480603, 443580, 443587, 4242669]],\n",
    "    ['ARFS', [197320, 192359]],\n",
    "    ['ESRD', [193782]],\n",
    "    ['Sepsis', [132797]],\n",
    "    ['Septic shock', [196236]],\n",
    "    ['Delirium', [373995]],\n",
    "    ['Altered mental status', [436222]],\n",
    "    ['Ventilation', [2106469, 2745444, 2788036, 2788037, 2788038, 46273390]],\n",
    "    ['Dead', [434489]],\n",
    "\n",
    "]\n",
    "\n",
    "# Show the concept definitions within each concept set to double check\n",
    "concept_set_names = list()\n",
    "concept_set_ids = list()\n",
    "for cs in concept_sets:\n",
    "    concept_set_names.append(cs[0])\n",
    "    concept_set_ids += cs[1]\n",
    "df_concept_set_definitions = df_concepts[df_concepts.index.isin(concept_set_ids)].copy()\n",
    "for cs in concept_sets:\n",
    "    df_concept_set_definitions.loc[df_concept_set_definitions.index.isin(cs[1]), 'concept_set'] = cs[0]\n",
    "df_concept_set_definitions.sort_values(by='concept_set', inplace=True)\n",
    "\n",
    "# Re-order, rename, and display\n",
    "df_concept_set_definitions.reset_index(inplace=True)  # show concept_id as a normal column\n",
    "df_concept_set_definitions = df_concept_set_definitions.loc[:, ['concept_set', 'concept_id', 'concept_name']]\n",
    "df_concept_set_definitions.columns = ['Concept Set', 'OMOP Concept ID', 'Concept Name']\n",
    "display(df_concept_set_definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concept_set_profiling(cpss, concept_sets):\n",
    "    cs_counter = Counter()\n",
    "    for cps in cpss:\n",
    "        # cps.sequence is a TaggedDoc with sequence of concepts stored as strings. \n",
    "        # Convert it to a list of concept_ids (ints)\n",
    "        seq = [int(x) for x in cps.sequence.words]\n",
    "\n",
    "        # Check if this patient has any of the concepts from each of the concept sets\n",
    "        for cs_name, cs_concepts in concept_sets:\n",
    "            for concept in cs_concepts:\n",
    "                if concept in seq:\n",
    "                    cs_counter[cs_name] += 1\n",
    "                    break           \n",
    "                    \n",
    "    return cs_counter\n",
    "\n",
    "def subtype_profiling_concept_sets(patients_learned_disease_subtype, concept_sets):\n",
    "    # dict[disease subtype] => dict[concept set (string)] => Counter\n",
    "    learned_disease_profiles = dict()\n",
    "\n",
    "    # For each disease subtype, count the number of patients each concept is observed in\n",
    "    for disease_subtype, cohort_cpss in patients_learned_disease_subtype.items():                    \n",
    "        learned_disease_profiles[disease_subtype] = concept_set_profiling(cohort_cpss, concept_sets)\n",
    "        \n",
    "    return learned_disease_profiles\n",
    "\n",
    "full_concept_set_profiles = concept_set_profiling(cpss.values(), concept_sets)\n",
    "subtype_concept_set_profiles = subtype_profiling_concept_sets(patients_learned_disease_subtype, concept_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _table_concept_set_subtype(row, custom_index, df_style=None, complement=True, adjustment=10000, show_n=False):\n",
    "    concept_set_name = row.name\n",
    "    cluster_index = subtype_custom_index_to_cluster_index[custom_index]\n",
    "    n = subtype_concept_set_profiles[cluster_index][concept_set_name] \n",
    "    n_disease_subtype = len(patients_learned_disease_subtype[cluster_index])\n",
    "    per = n / n_disease_subtype * 100\n",
    "    \n",
    "    if show_n:\n",
    "        s = f'{n} ({per:.01f}%)'\n",
    "    else:\n",
    "        s = f'{per:.01f}%'\n",
    "        \n",
    "    if df_style is not None:\n",
    "        # chi-square    \n",
    "        x = full_concept_set_profiles[concept_set_name]\n",
    "        obs_subtype = np.array([n, n_disease_subtype - n])\n",
    "        obs_comparator = np.array([x, n_patients - x])  # full cohort frequency\n",
    "        if complement:\n",
    "            # use complementary cohort frequency\n",
    "            obs_comparator = obs_comparator - obs_subtype\n",
    "        obs = np.array([obs_subtype, obs_comparator])\n",
    "        _, p, _, _ = chi2_contingency(obs)\n",
    "\n",
    "        sty = 'color: black'\n",
    "        if p < (0.05 / adjustment):\n",
    "            sty = 'font-weight: bold;'\n",
    "            if per < (x / n_patients * 100):\n",
    "                sty += 'color: blue'\n",
    "            else:\n",
    "                sty += 'color: red'        \n",
    "        df_style.loc[concept_set_name, subtype_cluster_index_to_custom_index[cluster_index]] = sty\n",
    "    \n",
    "    return s\n",
    "\n",
    "df = pd.DataFrame(data=None, index=concept_set_names, columns=['Full']+[x for x in range(1, n_clusters+1)])\n",
    "df_style = pd.DataFrame(data=None, index=concept_set_names, columns=['Full']+[x for x in range(1, n_clusters+1)])\n",
    "\n",
    "# Add concept set prevalence of the full cohort\n",
    "df['Full'] = df.apply(lambda row: f'{full_concept_set_profiles[row.name] / n_patients * 100:.01f}%', axis=1)\n",
    "df_style['Full'] = 'color: black'\n",
    "\n",
    "for i in range(1, n_clusters+1):\n",
    "    # Add prevalence and get styling for significance         \n",
    "    df[i] = df.apply(lambda row: _table_concept_set_subtype(row, i, df_style), axis=1)\n",
    "\n",
    "# Add N for full cohort and each subtype\n",
    "df.loc['Count', :] = [n_patients] + [len(patients_learned_disease_subtype[subtype_custom_index_to_cluster_index[i]]) for i in range(1, n_clusters+1)]\n",
    "# Change the order so that the \"Count\" row comes first\n",
    "df = df.reindex(['Count'] + concept_set_names)\n",
    "df_style.loc['Count', :] = 'color: black'\n",
    "\n",
    "# Add severity - single row\n",
    "row_severity = [f'{severity_full_pct.Moderate:.01f}%\\n{severity_full_pct.Severe:.01f}%\\n{severity_full_pct.Critical:.01f}%']\n",
    "row_severity_style = ['color:black']\n",
    "severity_score_full = severity_score(severity_full_pct)\n",
    "alpha = 0.05 / 10000\n",
    "for i in range(1, n_clusters+1):\n",
    "    cluster_index = subtype_custom_index_to_cluster_index[i]\n",
    "    t_severity_cnts = subtype_severity_cnts[cluster_index]\n",
    "    t_severity_pcts = subtype_severity_pcts[cluster_index]    \n",
    "    sig = _chi_freq(np.array(t_severity_cnts), np.array(severity_full_cnt), alpha)\n",
    "    row_severity += [f'{t_severity_pcts[0]:.01f}%\\n{t_severity_pcts[1]:.01f}%\\n{t_severity_pcts[2]:.01f}%']\n",
    "    s = 'color: black'\n",
    "    if sig:\n",
    "        s = 'font-weight: bold;'\n",
    "        severity_score_sub = severity_score(t_severity_pcts)\n",
    "        if severity_score_sub > severity_score_full:\n",
    "            s += 'color: red'\n",
    "        else:\n",
    "            s += 'color: blue'\n",
    "    row_severity_style.append(s)\n",
    "df.loc['Mild-moderate\\nSevere\\nCritical', :] = row_severity\n",
    "df_style.loc['Mild-moderate\\nSevere\\nCritical', :] = row_severity_style\n",
    "\n",
    "# Apply styling\n",
    "df_style = df_style.reindex(df.index)\n",
    "df_styler = df.style.apply(lambda x: df_style, axis=None).applymap(lambda x: 'white-space: pre-wrap')\n",
    "df_styler = df_styler.applymap_index(lambda x: 'white-space: pre-wrap')\n",
    "\n",
    "# Save\n",
    "display(df_styler)\n",
    "df_styler.to_excel(path.join(dir_output, 'table-2-concept-set-prevalences-covid.xlsx'))\n",
    "df_styer_table_concept_sets_covid = df_styler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 3. Pre-COVID-19 subgroup concept prevalences using concept sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_sets_pre = [\n",
    "    [\"Obesity\", [433736, 434005]],\n",
    "    [\"Essential hypertension\", [320128]],\n",
    "    [\"Hyperlipidemia\", [432867, 438720]],\n",
    "    [\"T2DM\", [4193704, 4193704, 201826, 43531578, 37016349, 443729, 443731, 376065, 45757363, 443733]],\n",
    "    [\"Pregnant\", [444094, 4218813, 4244438, 4307820, 4188598, 4239938, 439658, 4084768, 4094910, 4174506, 4097608, 4197245, 4132434, 4181751, 443874, 441678, 442558, 444267, 4051642, 4274955, 433864, 434484, 444461, 4185780, 4336226, 444417, 438543, 438542, 432430, 444023, 435640, 4322726, 4266517, 4242241, 4049621, 4277749, 4283690]],\n",
    "    [\"Constipation\", [75860]],\n",
    "    [\"Diarrhea\", [196523]],\n",
    "    [\"Cough\", [254761]],\n",
    "    [\"Dyspnea\", [312437]],\n",
    "    [\"COPD\", [255573]],\n",
    "    [\"Pleural effusion\", [254061]],    \n",
    "    [\"Atherosclerosis of coronary artery\", [764123]],\n",
    "    [\"Heart disease\", [764123, 313217, 314666, 4154290, 321319, 45766207, 4152384, 321588, 4270024, 319844, 4289309, 37312532] + [319835, 316139, 4229440, 40479192, 40479576, 443580, 40480602]],\n",
    "    [\"ARFS\", [197320, 192359]],\n",
    "    [\"CKD\", [46271022, 43531578, 443597, 45768812, 443612, 443611]],\n",
    "    [\"ESRD\", [193782]],\n",
    "    [\"Vitamin D deficiency\", [436070]],\n",
    "    [\"Immunodeficiency disorder\", [433740]],\n",
    "    [\"Dementia\", [4182210, 374888]],\n",
    "    [\"SOT\", [42539502, 42538117, 42539698, 42537742]]\n",
    "]\n",
    "\n",
    "# Show the concept definitions within each concept set to double check\n",
    "concept_set_names_pre = list()\n",
    "concept_set_ids_pre = list()\n",
    "for cs in concept_sets_pre:\n",
    "    concept_set_names_pre.append(cs[0])\n",
    "    concept_set_ids_pre += cs[1]\n",
    "df_concept_set_definitions_pre = df_concepts[df_concepts.index.isin(concept_set_ids_pre)].copy()\n",
    "for cs in concept_sets_pre:\n",
    "    df_concept_set_definitions_pre.loc[df_concept_set_definitions_pre.index.isin(cs[1]), 'concept_set'] = cs[0]\n",
    "df_concept_set_definitions_pre.sort_values(by='concept_set', inplace=True)\n",
    "display(df_concept_set_definitions_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_concept_set_profiles_pre = concept_set_profiling(cpss_pre.values(), concept_sets_pre)\n",
    "subtype_concept_set_profiles_pre = subtype_profiling_concept_sets(patients_learned_disease_subtype_pre, concept_sets_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _table_concept_set_subtype_pre(row, custom_index, df_style=None, complement=True, adjustment=10000, show_n=False):\n",
    "    concept_set_name = row.name\n",
    "    cluster_index = subtype_custom_index_to_cluster_index[custom_index]\n",
    "    n = subtype_concept_set_profiles_pre[cluster_index][concept_set_name] \n",
    "    n_disease_subtype = len(patients_learned_disease_subtype[cluster_index])\n",
    "    per = n / n_disease_subtype * 100\n",
    "    \n",
    "    if show_n:\n",
    "        s = f'{n} ({per:.01f}%)'\n",
    "    else:\n",
    "        s = f'{per:.01f}%'\n",
    "        \n",
    "    if df_style is not None:\n",
    "        # chi-square    \n",
    "        x = full_concept_set_profiles_pre[concept_set_name]\n",
    "        obs_subtype = np.array([n, n_disease_subtype - n])\n",
    "        obs_comparator = np.array([x, n_patients - x])  # full cohort frequency\n",
    "        if complement:\n",
    "            # use complementary cohort frequency\n",
    "            obs_comparator = obs_comparator - obs_subtype\n",
    "        obs = np.array([obs_subtype, obs_comparator])\n",
    "        try:\n",
    "            _, p, _, _ = chi2_contingency(obs)\n",
    "        except ValueError:\n",
    "            print(concept_set_name)\n",
    "            print(custom_index)\n",
    "\n",
    "        sty = 'color: black'\n",
    "        if p < (0.05 / adjustment):\n",
    "            sty = 'font-weight: bold;'\n",
    "            if per < (x / n_patients * 100):\n",
    "                sty += 'color: blue'\n",
    "            else:\n",
    "                sty += 'color: red'        \n",
    "        df_style.loc[concept_set_name, subtype_cluster_index_to_custom_index[cluster_index]] = sty\n",
    "    \n",
    "    return s\n",
    "\n",
    "df = pd.DataFrame(data=None, index=concept_set_names_pre, columns=['Full']+[x for x in range(1, n_clusters+1)])\n",
    "df_style = pd.DataFrame(data=None, index=concept_set_names_pre, columns=['Full']+[x for x in range(1, n_clusters+1)])\n",
    "\n",
    "# Add concept set prevalence of the full cohort\n",
    "df['Full'] = df.apply(lambda row: f'{full_concept_set_profiles_pre[row.name] / n_patients * 100:.01f}%', axis=1)\n",
    "df_style['Full'] = 'color: black'\n",
    "\n",
    "for i in range(1, n_clusters+1):\n",
    "    # Add prevalence and get styling for significance         \n",
    "    df[i] = df.apply(lambda row: _table_concept_set_subtype_pre(row, i, df_style), axis=1)\n",
    "\n",
    "# Add N for full cohort and each subtype\n",
    "df.loc['Count', :] = [n_patients] + [len(patients_learned_disease_subtype[subtype_custom_index_to_cluster_index[i]]) for i in range(1, n_clusters+1)]\n",
    "# Change the order so that the \"Count\" row comes first\n",
    "df = df.reindex(['Count'] + concept_set_names_pre)\n",
    "df_style.loc['Count', :] = 'color: black'\n",
    "\n",
    "# Apply styling\n",
    "df_style = df_style.reindex(df.index)\n",
    "df_styler = df.style.apply(lambda x: df_style, axis=None).applymap(lambda x: 'white-space: pre-wrap')\n",
    "\n",
    "# Save\n",
    "display(df_styler)\n",
    "df_styler.to_excel(path.join(dir_output, 'table-3-concept-set-prevalences-pre-covid.xlsx'))\n",
    "df_styer_table_concept_sets_pre = df_styler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplemental Tables - Subgroup COVID and pre-COVID conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the stats of each concept that is observed in > X% of each disease subtype\n",
    "print('Note: subtype percentages can add up to > 100% since different timelines can be extracted from each patient to represent a disease subtype')\n",
    "adjustment = 10**6\n",
    "prevalence_threshold_analysis = 10\n",
    "prevalence_threshold_display = 10\n",
    "prevalence_threshold_analysis_pre = 10\n",
    "prevalence_threshold_display_pre = 10\n",
    "count_threshold_display = 10\n",
    "df_conditions_covid = None\n",
    "df_conditions_precovid = None\n",
    "complement = True\n",
    "\n",
    "for disease_subtype in range(1, n_clusters+1):\n",
    "    cluster_index = subtype_custom_index_to_cluster_index[disease_subtype]\n",
    "    cohort_domain_concept_counter = learned_disease_profiles[cluster_index]\n",
    "    cohort_domain_concept_counter_pre = learned_disease_profiles_pre[cluster_index]\n",
    "    n_disease_subtype = len(patients_learned_disease_subtype[cluster_index])\n",
    "    n_comparator = n_patients - n_disease_subtype if complement else n_patients\n",
    "    \n",
    "    for domain in ['Condition']:        \n",
    "        ################################## COVID ##################################\n",
    "        concept_counter = cohort_domain_concept_counter[domain]        \n",
    "        \n",
    "        # Create a DataFrame from the counts\n",
    "        df_subtype_domain_counts = pd.DataFrame.from_dict(concept_counter, orient='index', columns=['count'])\n",
    "        df_subtype_domain_counts.index.name = 'concept_id'\n",
    "        \n",
    "        # Calculate prevalence of each concept within the disease subtype\n",
    "        df_subtype_domain_counts['subcohort prevalence'] = df_subtype_domain_counts['count'] / n_disease_subtype * 100\n",
    "        \n",
    "        # Add concept_name to DataFrame\n",
    "        df_subtype_domain_counts = df_subtype_domain_counts.join(df_concepts['concept_name'], how='left')\n",
    "        \n",
    "        # Re-arrange column order,  sort by descending order of count, and display\n",
    "        df_subtype_domain_counts = df_subtype_domain_counts[['concept_name', 'count', 'subcohort prevalence']]        \n",
    "        df_subtype_domain_counts = df_subtype_domain_counts.loc[((df_subtype_domain_counts['subcohort prevalence'] > prevalence_threshold_analysis)), :]\n",
    "        df_subtype_domain_counts['COVID-19 prevalence'], df_subtype_domain_counts['P value'], df_subtype_domain_counts['sig'] = [0, 0, 0]\n",
    "\n",
    "        # Chi-square        \n",
    "        for index, row in df_subtype_domain_counts.iterrows():\n",
    "            o = row['count']\n",
    "            x = known_disease_profiles[37311061][domain][index]            \n",
    "            if complement:\n",
    "                # compare against complementary cohort\n",
    "                x -= o\n",
    "            obs = np.array([[o, n_disease_subtype - o], [x, n_comparator - x]])\n",
    "            g, p, dof, exp = chi2_contingency(obs)\n",
    "            full_prev = x / n_comparator * 100.0\n",
    "            df_subtype_domain_counts.loc[index, 'full prev'] = full_prev\n",
    "            df_subtype_domain_counts.loc[index, 'COVID-19 prevalence'] = f'{x} ({full_prev:.01f}%)' \n",
    "            df_subtype_domain_counts.loc[index, 'P value'] = (f'{p:.02e}')\n",
    "            df_subtype_domain_counts.loc[index, 'sig'] = p < (0.05 / adjustment)   \n",
    "                \n",
    "        # Filter\n",
    "        display_filter = (df_subtype_domain_counts['count'] >= count_threshold_display) & \\\n",
    "                (df_subtype_domain_counts['subcohort prevalence'] > prevalence_threshold_display) & (df_subtype_domain_counts['sig'])\n",
    "        df_subtype_domain_counts = df_subtype_domain_counts.loc[display_filter, :]                \n",
    "                \n",
    "        df_subtype_domain_counts['Subgroup prevalence'] = df_subtype_domain_counts['count'].apply(lambda x: f'{int(x)} ({x / n_disease_subtype * 100.0:.01f}%)')\n",
    "        \n",
    "        # Add the count to the beginning\n",
    "        if len(df_subtype_domain_counts) > 0:\n",
    "            df_subtype_domain_counts.loc[0, ['concept_name', 'count', 'Subgroup prevalence', 'COVID-19 prevalence', 'P value']] = \\\n",
    "                ['Count', int(n_disease_subtype), f'{int(n_disease_subtype)} (100%)', f'{n_comparator} (100%)', '']\n",
    "            \n",
    "        df_subtype_domain_counts.sort_values(by='count', ascending=False, inplace=True)\n",
    "        df_subtype_domain_counts['Subgroup'] = disease_subtype\n",
    "        \n",
    "        if df_conditions_covid is None:\n",
    "            df_conditions_covid = df_subtype_domain_counts\n",
    "        else:\n",
    "            df_conditions_covid = pd.concat([df_conditions_covid, df_subtype_domain_counts])\n",
    "        \n",
    "        \n",
    "        ################################## pre-COVID ##################################\n",
    "        concept_counter = cohort_domain_concept_counter_pre[domain]        \n",
    "        \n",
    "        # Create a DataFrame from the counts\n",
    "        df_subtype_domain_counts = pd.DataFrame.from_dict(concept_counter, orient='index', columns=['count'])\n",
    "        df_subtype_domain_counts.index.name = 'concept_id'\n",
    "        \n",
    "        # Calculate prevalence of each concept within the disease subtype\n",
    "        df_subtype_domain_counts['subcohort prevalence'] = df_subtype_domain_counts['count'] / n_disease_subtype * 100\n",
    "        \n",
    "        # Add concept_name to DataFrame\n",
    "        df_subtype_domain_counts = df_subtype_domain_counts.join(df_concepts['concept_name'], how='left')\n",
    "        \n",
    "        # Re-arrange column order,  sort by descending order of count, and display\n",
    "        df_subtype_domain_counts = df_subtype_domain_counts[['concept_name', 'count', 'subcohort prevalence']]\n",
    "        df_subtype_domain_counts = df_subtype_domain_counts.loc[((df_subtype_domain_counts['subcohort prevalence'] > prevalence_threshold_analysis_pre)), :]\n",
    "        df_subtype_domain_counts['COVID-19 prevalence'], df_subtype_domain_counts['P value'], df_subtype_domain_counts['sig'] = [0, 0, 0]\n",
    "        \n",
    "        # Chi-square        \n",
    "        for index, row in df_subtype_domain_counts.iterrows():\n",
    "            o = row['count']\n",
    "            x = known_disease_profiles_pre[37311061][domain][index]\n",
    "            if complement:\n",
    "                # compare against complementary cohort\n",
    "                x -= o\n",
    "            obs = np.array([[o, n_disease_subtype - o], [x, n_comparator - x]])\n",
    "            g, p, dof, exp = chi2_contingency(obs)\n",
    "            full_prev = x / n_comparator * 100.0\n",
    "            df_subtype_domain_counts.loc[index, 'full prev'] = full_prev\n",
    "            df_subtype_domain_counts.loc[index, 'COVID-19 prevalence'] = f'{x} ({full_prev:.01f}%)' \n",
    "            df_subtype_domain_counts.loc[index, 'P value'] = (f'{p:.02e}')\n",
    "            df_subtype_domain_counts.loc[index, 'sig'] = p < (0.05 / adjustment)     \n",
    "                                       \n",
    "        # Filter        \n",
    "        display_filter = (df_subtype_domain_counts['count'] >= count_threshold_display) & \\\n",
    "                ((df_subtype_domain_counts['subcohort prevalence'] > prevalence_threshold_display_pre) & (df_subtype_domain_counts['sig']))\n",
    "        df_subtype_domain_counts = df_subtype_domain_counts.loc[display_filter, :]\n",
    "                                  \n",
    "        df_subtype_domain_counts['Subgroup prevalence'] = df_subtype_domain_counts['count'].apply(lambda x: f'{int(x)} ({x / n_disease_subtype * 100.0:.01f}%)')\n",
    "        \n",
    "        # Add the count to the beginning\n",
    "        if len(df_subtype_domain_counts) > 0:\n",
    "            df_subtype_domain_counts.loc[0, ['concept_name', 'count', 'Subgroup prevalence', 'COVID-19 prevalence', 'P value']] = \\\n",
    "                ['Count', int(n_disease_subtype), f'{int(n_disease_subtype)} (100%)', f'{n_comparator} (100%)', '']\n",
    "        \n",
    "        df_subtype_domain_counts.sort_values(by='count', ascending=False, inplace=True)  \n",
    "        df_subtype_domain_counts['Subgroup'] = disease_subtype\n",
    "        \n",
    "        if df_conditions_precovid is None:\n",
    "            df_conditions_precovid = df_subtype_domain_counts\n",
    "        else:\n",
    "            df_conditions_precovid = pd.concat([df_conditions_precovid, df_subtype_domain_counts])           \n",
    "\n",
    "df_conditions_covid.rename(columns={'concept_name': 'Condition'}, inplace=True)\n",
    "df_conditions_precovid.rename(columns={'concept_name': 'Condition'}, inplace=True)\n",
    "if complement:\n",
    "    df_conditions_covid.rename(columns={'COVID-19 prevalence': 'Complement prevalence'}, inplace=True)\n",
    "    df_conditions_precovid.rename(columns={'COVID-19 prevalence': 'Complement prevalence'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show certain columns, don't show COVID-19 (37311061)\n",
    "df = df_conditions_covid[['Subgroup', 'Condition', 'Subgroup prevalence', 'Complement prevalence', 'P value', 'subcohort prevalence', 'full prev']].drop(index=37311061)\n",
    "display(df.head())\n",
    "df.to_excel(path.join(dir_output, 'table-subgroup-conditions-covid.xlsx'))\n",
    "print(len(df_conditions_covid))\n",
    "\n",
    "# Show certain columns\n",
    "df = df_conditions_precovid[['Subgroup', 'Condition', 'Subgroup prevalence', 'Complement prevalence', 'P value', 'subcohort prevalence', 'full prev']]\n",
    "display(df.head())\n",
    "df.to_excel(path.join(dir_output, 'table-subgroup-conditions-precovid.xlsx'))\n",
    "print(len(df_conditions_precovid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Table 3. discharge diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diag = 3\n",
    "df_table_discharge_diag = pd.DataFrame(index=list(range(1, n_clusters+1)), columns=list(range(1, n_diag+1)))\n",
    "for i in range(1, n_clusters+1):\n",
    "    cluster_index = subtype_custom_index_to_cluster_index[i]\n",
    "    df_table_discharge_diag.loc[i, :] = subtype_discharge_diagnoses[cluster_index].head(n_diag).apply(lambda r: f'{r[\"concept_name\"]} ({r[\"percent\"]:.01f}%)', axis=1).tolist()\n",
    "\n",
    "display(df_table_discharge_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate hold-outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of distances of each vector to cluster center\n",
    "cluster_distances_train = defaultdict(list)\n",
    "for i in range(n_patients):\n",
    "    cluster = predicted_labels[i]\n",
    "    vector = patient_vectors[i]\n",
    "    dist = np.linalg.norm(clustering.cluster_centers_[cluster] - vector)\n",
    "    cluster_distances_train[cluster].append(dist)\n",
    "    \n",
    "for i in range(1, n_clusters+1):\n",
    "    cluster_index = subtype_custom_index_to_cluster_index[i]\n",
    "    ds = cluster_distances_train[cluster_index]\n",
    "    q = np.quantile(ds, [.25, .50, .75, .95])\n",
    "    print(f'{i} ({len(ds)}): {q}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_evaluation(patient_ids, clustering, cluster_distances_train, dist_percentile=.95, print_quantiles=True):\n",
    "    df = pd.DataFrame(data=0, index=list(range(1, clustering.n_clusters+1)) + ['Full', 'P-value'], columns=['Outliers'])\n",
    "    cluster_distances = defaultdict(list)\n",
    "    vectors = np.array([patient_vectors_dict_orig[pid] for pid in patient_ids])\n",
    "    predicted_labels = clustering.predict(vectors)\n",
    "    n_patients_holdout = len(patient_ids)\n",
    "    \n",
    "    # Calculate distances of holdout vectors to the cluster centers\n",
    "    for v, l in zip(vectors, predicted_labels):\n",
    "        dist = np.linalg.norm(clustering.cluster_centers_[l] - v)\n",
    "        cluster_distances[l].append(dist)\n",
    "    \n",
    "    # See how many distances are farther than the 95th percentile distance of each cluster from the training set\n",
    "    far = np.zeros(clustering.n_clusters)\n",
    "    far_train = np.zeros(clustering.n_clusters)\n",
    "    n_train = 0\n",
    "    for i in range(1, n_clusters+1):        \n",
    "        # Get distances and count of outliers from training\n",
    "        cluster_index = subtype_custom_index_to_cluster_index[i]                \n",
    "        cdt_i = cluster_distances_train[cluster_index]\n",
    "        dist_thresh = np.quantile(cdt_i, dist_percentile)\n",
    "        far_train[i-1] = np.sum(np.array(cdt_i) > dist_thresh)\n",
    "        n_train += len(cdt_i)\n",
    "        \n",
    "        ds = cluster_distances[cluster_index]\n",
    "        n_subtype = len(ds)        \n",
    "        if n_subtype == 0:\n",
    "            print(f'{i}: no representation')\n",
    "            df.loc[i, 'Outliers'] = '0 / 0 (NA)'\n",
    "            continue\n",
    "        \n",
    "        if print_quantiles:\n",
    "            q = np.quantile(ds, [.25, .50, .75, .95])\n",
    "            print(f'{i} ({n_subtype}): {q}')\n",
    "        \n",
    "        # Count outliers from holdout\n",
    "        f_i = np.sum(np.array(ds) > dist_thresh)\n",
    "        far[i-1] = f_i        \n",
    "        s = f'{f_i} / {n_subtype} ({f_i / n_subtype * 100:.01f}%)'\n",
    "        df.loc[i, 'Outliers'] = s\n",
    "        \n",
    "    f_all = int(np.sum(far))\n",
    "    f_train = int(np.sum(far_train))\n",
    "    s = f'{f_all} / {n_patients_holdout} ({f_all / n_patients_holdout * 100:.01f}%)'\n",
    "    print(f'all: {s}')    \n",
    "    df.loc['Full', 'Outliers'] = s\n",
    "    \n",
    "    # chi-square    \n",
    "    obs = np.array([[f_all, n_patients_holdout - f_all], \n",
    "                    [round(.05*n_patients_holdout), round(.95*n_patients_holdout)]])\n",
    "    _, p, _, _ = chi2_contingency(obs)\n",
    "    print(f'Full p-value: {p:.04f}')    \n",
    "    df.loc['P-value', 'Outliers'] = p\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# in-time holdout\n",
    "print('In-time holdout')\n",
    "df_holdout_it = holdout_evaluation(patient_ids_holdout_it, clustering, cluster_distances_train, print_quantiles=False)\n",
    "df_holdout_it.columns = ['In-time Holdout']\n",
    "print()\n",
    "\n",
    "# out-of-time holdout\n",
    "print('Out-of-time holdout')\n",
    "df_holdout_oot = holdout_evaluation(patient_ids_holdout_oot, clustering, cluster_distances_train, print_quantiles=False)\n",
    "df_holdout_oot.columns = ['Out-of-time Holdout']\n",
    "print()\n",
    "\n",
    "# combined holdout\n",
    "print('Combined holdout')\n",
    "df_holdout_combined = holdout_evaluation(patient_ids_holdout_it + patient_ids_holdout_oot, clustering, cluster_distances_train, print_quantiles=False)\n",
    "df_holdout_combined.columns = ['Combined Holdout']\n",
    "\n",
    "df_holdout = pd.concat([df_holdout_it, df_holdout_oot, df_holdout_combined], axis=1)\n",
    "display(df_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient Clustering Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patient_vector_inference(model_dm, model_dbow, sequence, n_inferences=1):\n",
    "    # Number of times to run inference on a patient sequence. Mean of inferred vectors will be used\n",
    "    n_inferences = 1\n",
    "    \n",
    "    l_dm = list()\n",
    "    l_dbow = list()\n",
    "\n",
    "    # Take the mean of n_inferences iterations of inferring the vector\n",
    "    for _ in range(n_inferences):\n",
    "        l_dm.append(model_dm.infer_vector(sequence.words))\n",
    "        l_dbow.append(model_dm.infer_vector(sequence.words))\n",
    "    a_dm = np.mean(np.array(l_dm), axis=0)\n",
    "    a_dbow = np.mean(np.array(l_dm), axis=0)\n",
    "    patient_vector = np.concatenate((a_dm, a_dbow))\n",
    "    patient_vector = np.array(patient_vector)\n",
    "    return patient_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_vector_sequences_window(f_pcs_in, df_events, model_dm, model_dbow, f_seq_out=None, time_window=[-14,28], randomize_order=True, random_seed=None, verbose=False, save_intermediates=False): \n",
    "    \"\"\" Reads the patient_code_sequences.txt file and extracts sequences within the time_window days around the \n",
    "    first occurrence of any encountered desired concept. Creates patient vectors on a day-by-day basis using\n",
    "    cummulative data from the beginning of the time window to the current date\n",
    "    \n",
    "    Note: save_intermediates makes it a lot slower \n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    dict[person_id] -> list(patient_vectors)\"\"\"\n",
    "\n",
    "    # For keeping track of processing time\n",
    "    t1 = time()\n",
    "\n",
    "    # pcss - cohort patient sequences: list of CohortPatientSeq objects\n",
    "    cohort_temporal_vectors = dict()\n",
    "    count = 0\n",
    "    \n",
    "    # Time window for finding occurrences \n",
    "    time_window_pre = timedelta(days=time_window[0])\n",
    "    time_window_post = timedelta(days=time_window[1])\n",
    "    \n",
    "    r = Random(random_seed)\n",
    "    \n",
    "    if f_seq_out:\n",
    "        f_intermediate = f_seq_out + '.tmp'\n",
    "    \n",
    "    # Read patient_code_sequences.txt\n",
    "    with open(f_pcs_in) as fh:  \n",
    "        # Skip the header line\n",
    "        fh.readline()\n",
    "        \n",
    "        for line in fh:\n",
    "            # Parse the line into person_id and list of date_occurrences\n",
    "            pid, date_occurrences = _process_pcs_line(line)\n",
    "            \n",
    "            if pid not in df_events.index:\n",
    "                # Couldn't find this person_id in the events table\n",
    "                continue\n",
    "                \n",
    "            # Get the index date from events table    \n",
    "            event_start_date = df_events.loc[pid, 'start_date']\n",
    "            date_lower = event_start_date + time_window_pre\n",
    "            date_upper = event_start_date + time_window_post\n",
    "            \n",
    "            current_seq = list()\n",
    "            patient_vectors = list()\n",
    "            for do in date_occurrences:\n",
    "                if do.date < date_lower:\n",
    "                    continue\n",
    "\n",
    "                if do.date > date_upper:\n",
    "                    # No more date_occurrences within the desired time window.                         \n",
    "                    break\n",
    "\n",
    "                # The date_occurrence is within the time_window. Add occurrences to seq\n",
    "                concepts = do.concept_ids\n",
    "                if randomize_order:\n",
    "                    # Randomize the order of concepts occurring on the same date. Shuffle is applied in place\n",
    "                    r.shuffle(concepts)\n",
    "                current_seq += concepts\n",
    "                \n",
    "                # Convert the sequence of OMOP concept IDs to TaggedDocument for D2V processing\n",
    "                tagged_doc_seq = TaggedDocument(words=[str(x) for x in current_seq], tags=[pid])   \n",
    "                \n",
    "                # Perform patient vector inferencing\n",
    "                patient_vector = patient_vector_inference(model_dm, model_dbow, tagged_doc_seq)         \n",
    "                patient_vectors.append(patient_vector)\n",
    "                             \n",
    "\n",
    "            cohort_temporal_vectors[pid] = patient_vectors\n",
    "\n",
    "            # Display progress\n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                if verbose: \n",
    "                    # Processing time\n",
    "                    ellapsed_time = (time() - t1) / 60\n",
    "                    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "\n",
    "                if save_intermediates and f_seq_out:\n",
    "                    # Save a backup copy of the data\n",
    "                    pickle.dump(cpss, open(f_intermediate, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)      \n",
    "\n",
    "    if f_seq_out:\n",
    "        # Save cohort temporal vectors         \n",
    "        pickle.dump(cohort_temporal_vectors, open(f_seq_out, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Delete the backup file\n",
    "        if save_intermediates and path.exists(f_intermediate):\n",
    "            os.remove(f_intermediate)\n",
    "\n",
    "    # Display overall processing time\n",
    "    ellapsed_time = (time() - t1) / 60\n",
    "    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "    \n",
    "    return cohort_temporal_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_vector_sequences_visit(f_pcs_in, df_events, model_dm, model_dbow, cummulative=True, f_seq_out=None, time_window=[0, 0], \n",
    "                                          randomize_order=True, random_seed=None, verbose=False, save_intermediates=False): \n",
    "    \"\"\" Reads the patient_code_sequences.txt file and extracts sequences within the patient's index event +/- time_window. \n",
    "    Creates patient vectors on a day-by-day basis using the index event from [start_date + time_window[0], end_date + time_window[1]]\n",
    "    cummulative data from the beginning of the time window to the current date\n",
    "    \n",
    "    Note: save_intermediates makes it a lot slower \n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    dict[person_id] -> list(patient_vectors)\"\"\"\n",
    "\n",
    "    # For keeping track of processing time\n",
    "    t1 = time()\n",
    "\n",
    "    # pcss - cohort patient sequences: list of CohortPatientSeq objects\n",
    "    cohort_temporal_vectors = dict()\n",
    "    count = 0\n",
    "    \n",
    "    # Time window for finding occurrences \n",
    "    time_window_pre = timedelta(days=time_window[0])\n",
    "    time_window_post = timedelta(days=time_window[1])\n",
    "    \n",
    "    r = Random(random_seed)\n",
    "    \n",
    "    if f_seq_out:\n",
    "        f_intermediate = f_seq_out + '.tmp'\n",
    "    \n",
    "    # Read patient_code_sequences.txt\n",
    "    with open(f_pcs_in) as fh:  \n",
    "        # Skip the header line\n",
    "        fh.readline()\n",
    "        \n",
    "        for line in fh:\n",
    "            # Parse the line into person_id and list of date_occurrences\n",
    "            pid, date_occurrences = _process_pcs_line(line)\n",
    "            \n",
    "            if pid not in df_events.index:\n",
    "                # Couldn't find this person_id in the events table\n",
    "                continue\n",
    "                \n",
    "            # Get the index date from events table    \n",
    "            event_start_date = df_events.loc[pid, 'start_date']\n",
    "            event_end_date = df_events.loc[pid, 'end_date']\n",
    "            date_lower = event_start_date + time_window_pre\n",
    "            date_upper = event_end_date + time_window_post\n",
    "            \n",
    "            current_seq = list()\n",
    "            patient_vectors = list()\n",
    "            for do in date_occurrences:            \n",
    "                if do.date < date_lower:\n",
    "                    continue\n",
    "\n",
    "                if do.date > date_upper:\n",
    "                    # No more date_occurrences within the desired time window.                         \n",
    "                    break\n",
    "                    \n",
    "                if not cummulative: \n",
    "                    # do not accumulate concepts from the beginning of the time range\n",
    "                    current_seq = list()\n",
    "\n",
    "                # The date_occurrence is within the time_window. Add occurrences to seq\n",
    "                concepts = do.concept_ids\n",
    "                if randomize_order:\n",
    "                    # Randomize the order of concepts occurring on the same date. Shuffle is applied in place\n",
    "                    r.shuffle(concepts)\n",
    "                current_seq += concepts\n",
    "                \n",
    "                # Convert the sequence of OMOP concept IDs to TaggedDocument for D2V processing\n",
    "                tagged_doc_seq = TaggedDocument(words=[str(x) for x in current_seq], tags=[pid])   \n",
    "                \n",
    "                # Perform patient vector inferencing\n",
    "                patient_vector = patient_vector_inference(model_dm, model_dbow, tagged_doc_seq)         \n",
    "                patient_vectors.append(patient_vector)\n",
    "                             \n",
    "\n",
    "            cohort_temporal_vectors[pid] = patient_vectors\n",
    "\n",
    "            # Display progress\n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                if verbose: \n",
    "                    # Processing time\n",
    "                    ellapsed_time = (time() - t1) / 60\n",
    "                    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "\n",
    "                if save_intermediates and f_seq_out:\n",
    "                    # Save a backup copy of the data\n",
    "                    pickle.dump(cpss, open(f_intermediate, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)      \n",
    "\n",
    "    if f_seq_out:\n",
    "        # Save cohort temporal vectors         \n",
    "        pickle.dump(cohort_temporal_vectors, open(f_seq_out, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Delete the backup file\n",
    "        if save_intermediates and path.exists(f_intermediate):\n",
    "            os.remove(f_intermediate)\n",
    "\n",
    "    # Display overall processing time\n",
    "    ellapsed_time = (time() - t1) / 60\n",
    "    print(f'{count} - {ellapsed_time:.01f} min')\n",
    "    \n",
    "    return cohort_temporal_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using cummulative MCS from hospitalization\n",
    "Each daily vector has data from admission up to the given date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create patient vectors using cummulative data from the beginning of the time window up to each day\n",
    "# This takes approximately 4 hours\n",
    "f_seq_out = path.join(dir_output, 'daily_vectors_visitrange_cummulative.pkl')\n",
    "cohort_temporal_vectors_cummulative = create_patient_vector_sequences_visit(file_pcs, df_events_orig, model_dm, model_dbow,\n",
    "                                                                f_seq_out=f_seq_out, cummulative=True, randomize_order=True, \n",
    "                                                                random_seed=42, verbose=True, save_intermediates=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for each daily vector\n",
    "cohort_predicted_labels_cummulative = {pid: [subtype_cluster_index_to_custom_index[i] for i in clustering.predict(vectors)] for pid, vectors in cohort_temporal_vectors_cummulative.items() if len(vectors) > 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get deaths\n",
    "sql = \"\"\"SELECT v.visit_occurrence_id, v.person_id\n",
    "FROM covid19_table_name covid\n",
    "JOIN visit_occurrence v ON covid.visit_occurrence_id = v.visit_occurrence_id\n",
    "WHERE v.discharge_to_concept_id = 4216643;\"\"\"\n",
    "df_visit_death = pd.read_sql(sql, engine).set_index('visit_occurrence_id')\n",
    "df_visit_death = df_events.join(df_visit_death, on='visit_occurrence_id', how='inner')\n",
    "display(df_visit_death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get \"discharge to\" information\n",
    "sql = \"\"\"WITH discharge AS (SELECT v.discharge_to_concept_id, COUNT(*) AS count\n",
    "FROM covid19_table_name covid\n",
    "JOIN visit_occurrence v ON covid.visit_occurrence_id = v.visit_occurrence_id\n",
    "GROUP BY v.discharge_to_concept_id)\n",
    "\n",
    "SELECT \n",
    "    cp.person_id, cp.visit_occurrence_id, v.discharge_to_concept_id, c.concept_name AS discharge_to,\n",
    "    CASE\n",
    "        WHEN discharge_to_concept_id IN (8536, 4021968, 4161979, 4146681, 44814650) THEN 'Discharged'\n",
    "        WHEN discharge_to_concept_id IN (8863, 8920, 8970, 8717, 8971, 8676, 38004519, 8827) THEN 'Care'\n",
    "        WHEN discharge_to_concept_id IN (4216643, 8546) THEN 'Death or Hospice'\n",
    "        ELSE 'ERROR!!!'\n",
    "    END as discharge_to_grouped\n",
    "FROM covid19_table_name cp\n",
    "JOIN visit_occurrence v ON cp.visit_occurrence_id = v.visit_occurrence_id\n",
    "JOIN concept c ON v.discharge_to_concept_id = c.concept_id;\"\"\"\n",
    "df_discharged_to = pd.read_sql(sql, engine).set_index('visit_occurrence_id')\n",
    "\n",
    "# Only keep the visit_occurrences in df_events_orig (1st event per patient)\n",
    "df_discharged_to = df_discharged_to[df_discharged_to.index.isin(df_events_orig.visit_occurrence_id)]\n",
    "df_discharged_to = df_discharged_to.reset_index().set_index('person_id')\n",
    "\n",
    "# Show counts of discharge_to\n",
    "display(df_discharged_to.discharge_to.value_counts())\n",
    "display(df_discharged_to.discharge_to_grouped.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_transitions(cohort_predicted_labels, df_discharged_to, n_clusters):\n",
    "    state_start = 0\n",
    "    state_end = n_clusters + 1\n",
    "    state_care = n_clusters + 2\n",
    "    state_death = n_clusters + 3\n",
    "    dict_discharge_to_to_state = {\n",
    "        'Discharged': state_end,\n",
    "        'Care': state_care,\n",
    "        'Death or Hospice': state_death\n",
    "    }\n",
    "        \n",
    "    transition_matrix = np.zeros([n_clusters+4, n_clusters+4], dtype=np.uint16)\n",
    "    duration_tracker = defaultdict(list)\n",
    "    for pid, labels in cohort_predicted_labels.items():\n",
    "        current_label = state_start\n",
    "        duration = 1\n",
    "        for label in labels:\n",
    "            if label != current_label:\n",
    "                # Count the transitions between states\n",
    "                transition_matrix[current_label, label] += 1\n",
    "                if current_label != state_start:\n",
    "                    # Keep track of duration in state before transition\n",
    "                    duration_tracker[(current_label, label)].append(duration)\n",
    "                current_label = label\n",
    "                duration = 1\n",
    "            else:\n",
    "                if current_label != state_start:\n",
    "                    duration += 1\n",
    "                \n",
    "        # Add transition to final state, e.g., discharged, additional care, or death/hospice\n",
    "        final_state = dict_discharge_to_to_state[df_discharged_to.loc[pid, 'discharge_to_grouped']]        \n",
    "        transition_matrix[current_label, final_state] += 1\n",
    "        duration_tracker[(current_label, final_state)].append(duration)                    \n",
    "    \n",
    "    # Get median of duration before transition between states\n",
    "    duration_matrix = np.zeros([n_clusters+4, n_clusters+4], dtype=np.uint16)\n",
    "    for transition, durations in duration_tracker.items():\n",
    "        duration_matrix[transition[0], transition[1]] = np.median(durations)\n",
    "        \n",
    "    return transition_matrix, duration_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_paths(cohort_predicted_labels, df_discharge_to, n_clusters):    \n",
    "    path_tracker = defaultdict(list)\n",
    "    for pid, labels in cohort_predicted_labels.items():\n",
    "        current_state = None\n",
    "        current_duration = None\n",
    "        patient_states = list()\n",
    "        state_durations = list()        \n",
    "        for label in labels:\n",
    "            if label != current_state:\n",
    "                if current_state != None and current_duration != None:\n",
    "                    patient_states.append(current_state)\n",
    "                    state_durations.append(current_duration)\n",
    "                current_state = label\n",
    "                current_duration = 1\n",
    "            else:\n",
    "                current_duration += 1\n",
    "                \n",
    "        patient_states.append(current_state)\n",
    "        state_durations.append(current_duration)\n",
    "        \n",
    "        # Append final state, e.g., discharge, additional care, or death/hospice\n",
    "        if type(df_discharge_to.loc[pid, 'discharge_to_grouped']) is not str:\n",
    "            print(pid)\n",
    "            print(df_discharge_to.loc[pid, 'discharge_to_grouped'])\n",
    "        patient_states.append(df_discharge_to.loc[pid, 'discharge_to_grouped'])        \n",
    "        state_durations.append(0)\n",
    "        \n",
    "        path_tracker[tuple(patient_states)].append(state_durations)\n",
    "        \n",
    "    return path_tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition characterization\n",
    "only patients with primary discharge diagnoses of COVID-19, sepsis, and viral pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patients_with_covid_discharge_diagnoses(df_events):\n",
    "    visit_occurrence_ids = df_events.visit_occurrence_id\n",
    "    sql = f'''SELECT co.person_id\n",
    "    FROM condition_occurrence co\n",
    "    WHERE co.condition_status_concept_id = 32903 -- primary discharge diagnosis\n",
    "        AND condition_concept_id IN (\n",
    "            37311061,  -- COVID-19\n",
    "            261326, -- Viral Pneumonia\n",
    "            132797 -- Sepsis\n",
    "        )\n",
    "        AND visit_occurrence_id IN ({','.join([str(i) for i in visit_occurrence_ids])});\n",
    "    '''\n",
    "    return pd.read_sql(sql, engine).person_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids_covid_disch_diag = patients_with_covid_discharge_diagnoses(df_events_orig)\n",
    "n_patients_cdd = len(patient_ids_covid_disch_diag)\n",
    "print(n_patients_cdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDD: covid discharge diagnoses\n",
    "cohort_predicted_labels_cummulative_cdd = {pid:labels for pid, labels in cohort_predicted_labels_cummulative.items() if pid in patient_ids_covid_disch_diag}\n",
    "transition_matrix_cdd, duration_matrix_cdd = count_transitions(cohort_predicted_labels_cummulative_cdd, df_discharged_to, n_clusters)\n",
    "col_names = ['admission'] + [str(i) for i in range(1, n_clusters+1)] + ['discharge', 'care', 'death']\n",
    "\n",
    "df_transition_matrix_cdd = pd.DataFrame(transition_matrix_cdd, columns=col_names, index=col_names)\n",
    "df_transition_matrix_cdd.to_csv(path.join(dir_output, 'transition_matrix_extra_discharge.csv'))\n",
    "\n",
    "df_duration_matrix_cdd = pd.DataFrame(duration_matrix_cdd, columns=col_names, index=col_names)\n",
    "df_duration_matrix_cdd.to_csv(path.join(dir_output, 'duration_matrix_extra_discharge.csv'))\n",
    "\n",
    "# Make a combined transition count and duration matrix for Supplementary Table\n",
    "df_trans_dur_matrix_cdd = pd.DataFrame('-', columns=col_names, index=col_names)\n",
    "for i in range(n_clusters+4):\n",
    "    for j in range(n_clusters+4):\n",
    "        if transition_matrix_cdd[i, j] >= 5:\n",
    "            df_trans_dur_matrix_cdd.iloc[i, j] = f'{transition_matrix_cdd[i, j]}, {duration_matrix_cdd[i, j]}'\n",
    "df_trans_dur_matrix_cdd = df_trans_dur_matrix_cdd.drop(labels=['discharge', 'care', 'death'], axis=0).drop(labels=['admission'], axis=1)\n",
    "display(df_trans_dur_matrix_cdd)\n",
    "df_trans_dur_matrix_cdd.to_excel(path.join(dir_output, 'trans_dur_matrix_extra_discharge.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Table config\n",
    "n_paths_table = 5\n",
    "n_states_max = 5\n",
    "\n",
    "path_tracker_cdd = count_paths(cohort_predicted_labels_cummulative_cdd, df_discharged_to, n_clusters)\n",
    "paths_cdd = list(path_tracker_cdd.keys())\n",
    "path_durations_cdd = list(path_tracker_cdd.values())\n",
    "n_paths_cdd = len(paths_cdd)\n",
    "\n",
    "# number of patients with each path\n",
    "path_prevalences_cdd = [len(path_tracker_cdd[path]) for path in paths_cdd]\n",
    "path_prev_argsort_cdd = np.argsort(path_prevalences_cdd)[::-1]\n",
    "\n",
    "# how many paths had more than X patients?\n",
    "n_x = 10\n",
    "path_x = [p for p in path_prevalences_cdd if p >= n_x]\n",
    "path_x_sum = np.sum(path_x)\n",
    "print(f'There are {len(path_x)} paths with at least {n_x} patients, totaling {path_x_sum} ({path_x_sum / n_patients_cdd * 100:.01f}%) patients')\n",
    "\n",
    "# quantiles of total duration of each path\n",
    "path_quantile_total_durations_cdd = np.zeros([n_paths_cdd], dtype=[('50', 'f4'), ('75', 'f4'), ('25', 'f4')])\n",
    "for i in range(n_paths_cdd):\n",
    "    path_quantile_total_durations_cdd[i] = tuple(np.quantile([np.sum(d) for d in path_durations_cdd[i]], [.50, .75, .25]))\n",
    "path_quantile_total_durations_argsort_cdd = np.argsort(path_quantile_total_durations_cdd, axis=0, order=['50', '75', '25'])[::-1]\n",
    "path_quantile_total_durations_dict_cdd = {paths_cdd[i]:path_quantile_total_durations_cdd[i] for i in range(n_paths_cdd)}\n",
    "\n",
    "# Mortality of base paths (base path = path composed of subgroups, i.e., excluding discharge/death)\n",
    "base_paths_cdd = list({p[:-1] for p in paths_cdd})\n",
    "n_base_paths = len(base_paths_cdd)\n",
    "base_paths_n_disch = [len(path_tracker_cdd[p + ('Discharged',)]) for p in base_paths_cdd]\n",
    "base_paths_n_care = [len(path_tracker_cdd[p + ('Care',)]) for p in base_paths_cdd]\n",
    "base_paths_n_death = [len(path_tracker_cdd[p + ('Death or Hospice',)]) for p in base_paths_cdd]\n",
    "base_paths_n_total = [base_paths_n_disch[i] + base_paths_n_care[i] + base_paths_n_death[i] for i in range(n_base_paths)]\n",
    "base_paths_death_rate = [base_paths_n_death[i] / base_paths_n_total[i] * 100 for i in range(n_base_paths)]\n",
    "base_paths_death_rate_dict = {base_paths_cdd[i]:f'{base_paths_n_death[i]} / {base_paths_n_total[i]} ({base_paths_death_rate[i]:.01f}%)' for i in range(n_base_paths)}\n",
    "base_paths_death_rate_argsort = np.argsort(base_paths_death_rate)\n",
    "\n",
    "# quantiles of total duration of each base path\n",
    "basepath_quantile_total_durations_cdd = np.zeros([n_base_paths], dtype=[('50', 'f4'), ('75', 'f4'), ('25', 'f4')])\n",
    "basepath_quantile_total_durations_dict_cdd = dict()\n",
    "for i, p in enumerate(base_paths_cdd):\n",
    "    combined_durations = path_tracker_cdd[p + ('Discharged', )] + path_tracker_cdd[p + ('Care', )] + path_tracker_cdd[p + ('Death or Hospice',)]\n",
    "    duration_quantile = tuple(np.quantile([np.sum(d) for d in combined_durations], [.50, .75, .25]))\n",
    "    basepath_quantile_total_durations_cdd[i] = duration_quantile\n",
    "    basepath_quantile_total_durations_dict_cdd[p] = duration_quantile\n",
    "basepath_quantile_total_durations_argsort_cdd = np.argsort(basepath_quantile_total_durations_cdd, axis=0, order=['50', '75', '25'])[::-1]\n",
    "\n",
    "# Create a table of the top N paths\n",
    "print(f'{len(paths_cdd)}\\n')\n",
    "\n",
    "# Paths with highest prevalence\n",
    "print('Highest Prevalence')\n",
    "df = pd.DataFrame(index=range(n_paths_table), columns=[f'State {i}' for i in range(1, n_states_max+1)] + ['Total Duration (days)', 'Count (%)', 'Mortality (%)'])\n",
    "df.fillna('', inplace=True)\n",
    "n_represented = 0\n",
    "for i in range(n_paths_table):\n",
    "    p = paths_cdd[path_prev_argsort_cdd[i]]\n",
    "    durations = path_tracker_cdd[p]\n",
    "    \n",
    "    # Get the median duration of each state\n",
    "    durations_median = [np.quantile([d[j] for d in durations], 0.5) for j in range(len(p))]\n",
    "    \n",
    "    # Show each state and duration\n",
    "    len_p = len(p)\n",
    "    for j in range(len_p - 1):\n",
    "        df.loc[i, f'State {j+1}'] = f'{p[j]} ({durations_median[j]} days)'\n",
    "    df.loc[i, f'State {len_p}'] = p[-1]\n",
    "    \n",
    "    # Number of patients\n",
    "    n = len(path_tracker_cdd[p])\n",
    "    df.loc[i, 'Count (%)'] = f'{n} ({n / n_patients_cdd * 100:.01f}%)'\n",
    "    n_represented += n\n",
    "    \n",
    "    # Mortality\n",
    "    df.loc[i, 'Mortality (%)'] = base_paths_death_rate_dict[p[:-1]]\n",
    "    \n",
    "    # Total duration\n",
    "    d = path_quantile_total_durations_dict_cdd[p]\n",
    "    df.loc[i, 'Total Duration (days)'] = f'{d[0]} [{d[2]}, {d[1]}]'\n",
    "    \n",
    "df_paths_highest_count = df\n",
    "display(df)\n",
    "print(f'number patients represented: {n_represented} / {n_patients_cdd} ({n_represented / n_patients_cdd * 100:.01f}%)\\n')\n",
    "\n",
    "# Paths with lowest mortality\n",
    "print('Lowest Mortality')\n",
    "count = 0\n",
    "df = pd.DataFrame(index=range(n_paths_table), columns=[f'State {i}' for i in range(1, n_states_max+1)] + ['Total Duration (days)', 'Count (%)', 'Mortality (%)'])\n",
    "df.fillna('', inplace=True)\n",
    "n_represented = 0\n",
    "\n",
    "for i in range(n_base_paths):    \n",
    "    if base_paths_n_total[base_paths_death_rate_argsort[i]] >= 50:\n",
    "        p_base = base_paths_cdd[base_paths_death_rate_argsort[i]]\n",
    "        p = p_base\n",
    "        \n",
    "        # Get the median duration of each state among base_path->discharge and base_path->care\n",
    "        durations = path_tracker_cdd[p_base + ('Discharged',)] + path_tracker_cdd[p_base + ('Care',)] + path_tracker_cdd[p_base + ('Death or Hospice',)]\n",
    "        durations_median = [np.quantile([d[j] for d in durations], 0.5) for j in range(len(p))]\n",
    "        n = len(durations)\n",
    "\n",
    "        # Show each state and duration\n",
    "        len_p = len(p)\n",
    "        for j in range(len_p):\n",
    "            df.loc[count, f'State {j+1}'] = f'{p[j]} ({durations_median[j]} days)'\n",
    "\n",
    "        # Number of patients        \n",
    "        df.loc[count, 'Count (%)'] = f'{n} ({n / n_patients_cdd * 100:.01f}%)'\n",
    "        n_represented += n\n",
    "        \n",
    "        # Total duration\n",
    "        d = basepath_quantile_total_durations_dict_cdd[p]\n",
    "        df.loc[count, 'Total Duration (days)'] = f'{d[0]} [{d[2]}, {d[1]}]'\n",
    "        \n",
    "        # Mortality\n",
    "        df.loc[count, 'Mortality (%)'] = base_paths_death_rate_dict[p_base]\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    if count >= n_paths_table:\n",
    "        break\n",
    "df_paths_lowest_death = df\n",
    "display(df)\n",
    "n_death = len(df_visit_death)\n",
    "print(f'number patients represented: {n_represented} / {n_patients_cdd} ({n_represented / n_patients_cdd * 100:.01f}%)\\n')\n",
    "    \n",
    "# Paths with highest mortality\n",
    "print('Highest Mortality')\n",
    "count = 0\n",
    "df = pd.DataFrame(index=range(n_paths_table), columns=[f'State {i}' for i in range(1, n_states_max+1)] + ['Total Duration (days)', 'Count (%)', 'Mortality (%)'])\n",
    "df.fillna('', inplace=True)\n",
    "n_represented = 0\n",
    "\n",
    "for i in range(n_base_paths - 1, -1, -1):  \n",
    "    if base_paths_n_total[base_paths_death_rate_argsort[i]] >= 10:\n",
    "        p_base = base_paths_cdd[base_paths_death_rate_argsort[i]]\n",
    "        p = p_base\n",
    "        \n",
    "        # Get the median duration of each state\n",
    "        durations = path_tracker_cdd[p_base + ('Discharged',)] + path_tracker_cdd[p_base + ('Care',)] + path_tracker_cdd[p_base + ('Death or Hospice',)]\n",
    "        durations_median = [np.quantile([d[j] for d in durations], 0.5) for j in range(len(p))]\n",
    "        n = len(durations)\n",
    "\n",
    "        # Show each state and duration\n",
    "        len_p = len(p)\n",
    "        for j in range(len_p):\n",
    "            df.loc[count, f'State {j+1}'] = f'{p[j]} ({durations_median[j]} days)'\n",
    "#         df.loc[count, f'State {len_p}'] = p[-1]\n",
    "\n",
    "        # Number of patients        \n",
    "        df.loc[count, 'Count (%)'] = f'{n} ({n / n_patients_cdd * 100:.01f}%)'\n",
    "        n_represented += n\n",
    "        \n",
    "        # Total duration\n",
    "        d = basepath_quantile_total_durations_dict_cdd[p]\n",
    "        df.loc[count, 'Total Duration (days)'] = f'{d[0]} [{d[2]}, {d[1]}]'\n",
    "        \n",
    "        # Mortality\n",
    "        df.loc[count, 'Mortality (%)'] = base_paths_death_rate_dict[p_base]\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    if count >= n_paths_table:\n",
    "        break\n",
    "df_paths_highest_death = df\n",
    "display(df)\n",
    "n_death = len(df_visit_death)\n",
    "print(f'number patients represented: {n_represented} / {n_death} ({n_represented / n_death * 100:.01f}%)\\n')\n",
    "    \n",
    "# Paths with longest median duration\n",
    "print('Longest Duration')\n",
    "count = 0\n",
    "df = pd.DataFrame(index=range(n_paths_table), columns=[f'State {i}' for i in range(1, n_states_max+1)] + ['Total Duration (days)', 'Count (%)', 'Mortality (%)'])\n",
    "df.fillna('', inplace=True)\n",
    "n_represented = 0\n",
    "for i in range(n_paths_cdd):\n",
    "    p = paths_cdd[path_quantile_total_durations_argsort_cdd[i]]\n",
    "    n = len(path_tracker_cdd[p])\n",
    "    if n >= 10: \n",
    "        durations = path_tracker_cdd[p]\n",
    "\n",
    "        # Get the median duration of each state\n",
    "        durations_median = [np.quantile([d[j] for d in durations], 0.5) for j in range(len(p))]\n",
    "\n",
    "        # Show each state and duration\n",
    "        len_p = len(p)\n",
    "        for j in range(len_p - 1):\n",
    "            df.loc[count, f'State {j+1}'] = f'{p[j]} ({durations_median[j]} days)'\n",
    "        df.loc[count, f'State {len_p}'] = p[-1]\n",
    "\n",
    "        # Number of patients    \n",
    "        df.loc[count, 'Count (%)'] = f'{n} ({n / n_patients_cdd * 100:.01f}%)'\n",
    "        n_represented += n\n",
    "        \n",
    "        # Total duration\n",
    "        d = path_quantile_total_durations_dict_cdd[p]\n",
    "        df.loc[count, 'Total Duration (days)'] = f'{d[0]} [{d[2]}, {d[1]}]'\n",
    "\n",
    "        # Mortality\n",
    "        df.loc[count, 'Mortality (%)'] = base_paths_death_rate_dict[p[:-1]]                \n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    if count >= n_paths_table:\n",
    "        break\n",
    "    \n",
    "df_paths_longest_duration = df\n",
    "display(df)\n",
    "print(f'number patients represented: {n_represented} / {n_patients_cdd} ({n_represented / n_patients_cdd * 100:.01f}%)\\n')\n",
    "\n",
    "pd.concat([df_paths_highest_count, df_paths_longest_duration, df_paths_lowest_death, df_paths_highest_death])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table of the top N paths\n",
    "n_paths_table = 70\n",
    "n_states_max = 5\n",
    "display_thresh = 10\n",
    "\n",
    "print(f'{len(paths_cdd)}\\n')\n",
    "\n",
    "# Paths with highest prevalence\n",
    "print('Highest Prevalence')\n",
    "df = pd.DataFrame(index=range(n_paths_table), columns=[f'State {i}' for i in range(1, n_states_max+1)] + ['Count (%)', 'Total Duration (days)', 'Mortality (%)'])\n",
    "n_represented = 0\n",
    "for i in range(n_paths_table):\n",
    "    p = paths_cdd[path_prev_argsort_cdd[i]]\n",
    "    n = len(path_tracker_cdd[p])\n",
    "    \n",
    "    if n < display_thresh:\n",
    "        break\n",
    "    \n",
    "    durations = path_tracker_cdd[p]\n",
    "    \n",
    "    # Get the median duration of each state\n",
    "    durations_median = [np.quantile([d[j] for d in durations], 0.5) for j in range(len(p))]\n",
    "    \n",
    "    # Show each state and duration\n",
    "    len_p = len(p)\n",
    "    for j in range(len_p - 1):\n",
    "        df.loc[i, f'State {j+1}'] = f'{p[j]} ({durations_median[j]} days)'\n",
    "    df.loc[i, f'State {len_p}'] = p[-1]\n",
    "    \n",
    "    # Number of patients    \n",
    "    df.loc[i, 'Count (%)'] = f'{n} ({n / n_patients_cdd * 100:.01f}%)'\n",
    "    n_represented += n\n",
    "    \n",
    "    # Mortality\n",
    "    df.loc[i, 'Mortality (%)'] = base_paths_death_rate_dict[p[:-1]]\n",
    "    \n",
    "    # Total duration\n",
    "    d = path_quantile_total_durations_dict_cdd[p]\n",
    "    df.loc[i, 'Total Duration (days)'] = f'{d[0]} [{d[1]}, {d[2]}]'\n",
    "    \n",
    "df = df.dropna(how='all').fillna('')\n",
    "df_paths_highest_count = df\n",
    "display(df)\n",
    "print(f'number patients represented: {n_represented} / {n_patients_cdd} ({n_represented / n_patients_cdd * 100:.01f}%)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many patients died when their first day was in a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 / n_clusters\n",
    "\n",
    "# General death \n",
    "n_death_gen = df_discharged_to.discharge_to_grouped.value_counts()['Death or Hospice']\n",
    "obs_death_gen = np.array([n_death_gen, n_patients_orig - n_death_gen])\n",
    "print(f'General Mortality: {n_death_gen} / {n_patients_orig} ({n_death_gen / n_patients_orig * 100:.01f}%)')\n",
    "\n",
    "# Characterize percent of deaths by state on first day \n",
    "df_admission_state_death = pd.DataFrame(index=['Full'] + list(range(n_clusters)), columns=['Death or Hospice', 'P-value'])\n",
    "df_admission_state_death.index.names = ['Group']\n",
    "df_admission_state_death.loc['Full', :] = [f'{n_death_gen}/{n_patients_orig} ({n_death_gen/n_patients_orig*100:.01f}%)', '']\n",
    "state_lowest = 1\n",
    "start_state_deaths = np.zeros(n_clusters, dtype='uint16')\n",
    "start_state_counts = np.zeros(n_clusters, dtype='uint16')\n",
    "count = 0\n",
    "for p, d in path_tracker.items():\n",
    "    n = len(d)\n",
    "    count += n\n",
    "    start_state_counts[p[0] - state_lowest] += n\n",
    "    if p[-1] == 'Death or Hospice':\n",
    "        start_state_deaths[p[0] - state_lowest] += n\n",
    "print(count)\n",
    "     \n",
    "count = 0\n",
    "for i in range(n_clusters):\n",
    "    d = start_state_deaths[i]\n",
    "    t = start_state_counts[i]\n",
    "    if t > 0:\n",
    "        # chi-square \n",
    "        obs_death_subtype = np.array([d, t-d])\n",
    "        obs = np.array([obs_death_subtype, obs_death_gen - obs_death_subtype])\n",
    "        _, p, _, _ = chi2_contingency(obs)\n",
    "        \n",
    "        df_admission_state_death.loc[i+state_lowest, ['Death or Hospice', 'P-value']] = [f'{d}/{t} ({d/t*100:.01f}%)', (f'{p:4.04f}' if p >= 0.0001 else '<0.0001') + ('*' if p < alpha else '')]\n",
    "        count += 1\n",
    "df_admission_state_death.dropna(inplace=True)\n",
    "display(df_admission_state_death)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
